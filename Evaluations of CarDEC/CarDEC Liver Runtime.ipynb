{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarDEC Liver Runtime\n",
    "\n",
    "In this notebook, we will analyze the scaleability of the CarDEC method to larger datasets. We fit CarDEC on various percentages of the Liver dataset (which contains over 100000 cells). We fit CarDEC on subsets of this data ranging from 10% up to 100% of the full liver data. We profile CarDEC both with and without the supplementary count space modeling (CarDEC Count is the runtime including count modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import RangeIndex\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Broadly useful python packages\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from shutil import move, rmtree\n",
    "import warnings\n",
    "from memory_profiler import memory_usage\n",
    "from time import time\n",
    "\n",
    "\"\"\"Machine learning and single cell packages\"\"\"\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import adjusted_rand_score as ari, normalized_mutual_info_score as nmi\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"CarDEC Package\"\"\"\n",
    "from CarDEC import CarDEC_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Miscellaneous useful functions\"\"\"\n",
    "\n",
    "def read_liver_data(path, cache=True):\n",
    "    adata = sc.read_mtx(os.path.join(path, 'matrix.mtx')).T\n",
    "    genes_file = pd.read_csv(os.path.join(path, 'genes.tsv'), sep='\\t')\n",
    "    barcodes_file = pd.read_csv(os.path.join(path, 'barcodes.tsv'), sep='\\t')\n",
    "\n",
    "    adata.var.index = genes_file[\"genename\"]\n",
    "    adata.obs.index = barcodes_file[\"cellname\"]\n",
    "    adata.obs = barcodes_file\n",
    "        \n",
    "    sc.pp.filter_cells(adata, min_genes = 200)\n",
    "    mito_genes = adata.var_names.str.startswith('mt-')\n",
    "    adata.obs['percent_mito'] = np.sum(\n",
    "        adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n",
    "    adata.obs['n_counts'] = adata.X.sum(axis=1).A1\n",
    "    adata = adata[adata.obs['percent_mito'] < 0.2, :]\n",
    "    sc.pp.filter_genes(adata, min_cells = 30)\n",
    "\n",
    "    return adata\n",
    "\n",
    "def build_dir(dir_path):\n",
    "    subdirs = [dir_path]\n",
    "    substring = dir_path\n",
    "\n",
    "    while substring != '':\n",
    "        splt_dir = os.path.split(substring)\n",
    "        substring = splt_dir[0]\n",
    "        subdirs.append(substring)\n",
    "        \n",
    "    subdirs.pop()\n",
    "    subdirs = [x for x in subdirs if os.path.basename(x) != '..']\n",
    "\n",
    "    n = len(subdirs)\n",
    "    subdirs = [subdirs[n - 1 - x] for x in range(n)]\n",
    "    \n",
    "    for dir_ in subdirs:\n",
    "        if not os.path.isdir(dir_):\n",
    "            os.mkdir(dir_)\n",
    "            \n",
    "def run_cardec(adata, include_counts):\n",
    "    CarDEC = CarDEC_API(adata, weights_dir = \"temp\", batch_key = \"sampleid\", n_high_var = 2000)\n",
    "    del adata\n",
    "    CarDEC.build_model(n_clusters = 11)\n",
    "    if include_counts:\n",
    "        CarDEC.make_inference(denoise_all = False)\n",
    "        CarDEC.model_counts()\n",
    "    else:\n",
    "        CarDEC.make_inference()\n",
    "        \n",
    "def profile(frac):\n",
    "    np.random.seed(11111)\n",
    "    indices = np.random.choice(range(adata.shape[0]), size = round(frac * adata.shape[0]), replace = False)\n",
    "    tmp = adata.copy()[indices]\n",
    "    \n",
    "    sc.pp.filter_genes(tmp, min_cells = 1)\n",
    "    start = time()\n",
    "    run = memory_usage((run_cardec, (), {'adata': tmp, 'include_counts': False}))\n",
    "    final = time() - start\n",
    "    peak_memory = max(run) - min(run)\n",
    "    stats_zscore = final, peak_memory, \"CarDEC Zscore\", int(100*frac)\n",
    "    rmtree(\"temp\")\n",
    "    \n",
    "    tmp = adata.copy()[indices]\n",
    "    \n",
    "    sc.pp.filter_genes(tmp, min_cells = 1)\n",
    "    start = time()\n",
    "    run = memory_usage((run_cardec, (), {'adata': tmp, 'include_counts': True}))\n",
    "    final = time() - start\n",
    "    peak_memory = max(run) - min(run)\n",
    "    stats_count = final, peak_memory, \"CarDEC Count\", int(100*frac)\n",
    "    rmtree(\"temp\")\n",
    "    \n",
    "    return stats_zscore , stats_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dir(\"../Figures/liver\")\n",
    "profile_stats = {\"Time (Seconds)\": [] , \"Memory (MiB)\": [], \"Method\": [], 'Percent': []}\n",
    "profile_stats = pd.DataFrame(profile_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming to str index.\n",
      "Trying to set attribute `.var` of view, copying.\n"
     ]
    }
   ],
   "source": [
    "adata = read_liver_data(\"../Data/liver\", cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Memory and Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.939, Validation Loss: 0.868, Time: 1.7 s\n",
      "Epoch 001: Training Loss: 0.847, Validation Loss: 0.824, Time: 1.6 s\n",
      "Epoch 002: Training Loss: 0.791, Validation Loss: 0.754, Time: 1.6 s\n",
      "Epoch 003: Training Loss: 0.757, Validation Loss: 0.744, Time: 1.6 s\n",
      "Epoch 004: Training Loss: 0.736, Validation Loss: 0.732, Time: 1.6 s\n",
      "Epoch 005: Training Loss: 0.718, Validation Loss: 0.708, Time: 1.6 s\n",
      "Epoch 006: Training Loss: 0.707, Validation Loss: 0.705, Time: 1.6 s\n",
      "Epoch 007: Training Loss: 0.699, Validation Loss: 0.691, Time: 1.6 s\n",
      "Epoch 008: Training Loss: 0.691, Validation Loss: 0.689, Time: 1.5 s\n",
      "Epoch 009: Training Loss: 0.685, Validation Loss: 0.677, Time: 1.5 s\n",
      "Epoch 010: Training Loss: 0.682, Validation Loss: 0.675, Time: 1.5 s\n",
      "Epoch 011: Training Loss: 0.679, Validation Loss: 0.681, Time: 1.5 s\n",
      "Epoch 012: Training Loss: 0.675, Validation Loss: 0.673, Time: 1.5 s\n",
      "Epoch 013: Training Loss: 0.672, Validation Loss: 0.678, Time: 1.5 s\n",
      "Epoch 014: Training Loss: 0.669, Validation Loss: 0.668, Time: 1.5 s\n",
      "Epoch 015: Training Loss: 0.665, Validation Loss: 0.670, Time: 1.5 s\n",
      "Epoch 016: Training Loss: 0.664, Validation Loss: 0.663, Time: 1.5 s\n",
      "Epoch 017: Training Loss: 0.662, Validation Loss: 0.669, Time: 1.5 s\n",
      "Epoch 018: Training Loss: 0.659, Validation Loss: 0.663, Time: 1.5 s\n",
      "Epoch 019: Training Loss: 0.661, Validation Loss: 0.666, Time: 1.5 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 020: Training Loss: 0.656, Validation Loss: 0.665, Time: 1.5 s\n",
      "Epoch 021: Training Loss: 0.656, Validation Loss: 0.657, Time: 1.5 s\n",
      "Epoch 022: Training Loss: 0.656, Validation Loss: 0.658, Time: 1.5 s\n",
      "Epoch 023: Training Loss: 0.655, Validation Loss: 0.662, Time: 1.5 s\n",
      "Epoch 024: Training Loss: 0.653, Validation Loss: 0.665, Time: 1.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 025: Training Loss: 0.656, Validation Loss: 0.653, Time: 1.5 s\n",
      "Epoch 026: Training Loss: 0.656, Validation Loss: 0.665, Time: 1.4 s\n",
      "Epoch 027: Training Loss: 0.653, Validation Loss: 0.679, Time: 1.5 s\n",
      "Epoch 028: Training Loss: 0.653, Validation Loss: 0.659, Time: 1.4 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 029: Training Loss: 0.652, Validation Loss: 0.658, Time: 1.5 s\n",
      "Epoch 030: Training Loss: 0.652, Validation Loss: 0.657, Time: 1.4 s\n",
      "Epoch 031: Training Loss: 0.654, Validation Loss: 0.671, Time: 1.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 032: Training Loss: 0.653, Validation Loss: 0.653, Time: 1.4 s\n",
      "Epoch 033: Training Loss: 0.652, Validation Loss: 0.668, Time: 1.5 s\n",
      "Epoch 034: Training Loss: 0.652, Validation Loss: 0.666, Time: 1.5 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 52.42 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/rp_tree.py\", line 135:\n",
      "@numba.njit(fastmath=True, nogil=True, parallel=True)\n",
      "def euclidean_random_projection_split(data, indices, rng_state):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/utils.py\", line 409:\n",
      "@numba.njit(parallel=True)\n",
      "def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n",
      "^\n",
      "\n",
      "  current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2499744   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2523304   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,545,880\n",
      "Trainable params: 5,545,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2495616   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,499,744\n",
      "Trainable params: 2,499,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2514984   \n",
      "=================================================================\n",
      "Total params: 2,523,304\n",
      "Trainable params: 2,523,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.914, Validation Cluster: 0.867, Validation AE: 0.737], Label Change: 0.058, Time: 12.1 s\n",
      "Iter 001 Loss: [Training: 0.966, Validation Cluster: 0.923, Validation AE: 0.735], Label Change: 0.011, Time: 12.6 s\n",
      "Iter 002 Loss: [Training: 0.987, Validation Cluster: 0.953, Validation AE: 0.736], Label Change: 0.010, Time: 12.3 s\n",
      "Iter 003 Loss: [Training: 0.994, Validation Cluster: 0.967, Validation AE: 0.737], Label Change: 0.006, Time: 12.2 s\n",
      "Iter 004 Loss: [Training: 0.994, Validation Cluster: 0.969, Validation AE: 0.734], Label Change: 0.007, Time: 12.2 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 005 Loss: [Training: 0.994, Validation Cluster: 0.980, Validation AE: 0.735], Label Change: 0.002, Time: 12.4 s\n",
      "Iter 006 Loss: [Training: 0.995, Validation Cluster: 0.985, Validation AE: 0.739], Label Change: 0.002, Time: 11.9 s\n",
      "Iter 007 Loss: [Training: 0.990, Validation Cluster: 0.993, Validation AE: 0.746], Label Change: 0.002, Time: 12.9 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.74598944 not improving.\n",
      "Proportion of Labels Changed:  0.0016238418187028369  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     2361\n",
      "1     2439\n",
      "2     1277\n",
      "3     1487\n",
      "4      694\n",
      "5      609\n",
      "6      734\n",
      "7      307\n",
      "8      218\n",
      "9      187\n",
      "10     156\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 180.72132301330566\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.939, Validation Loss: 0.868, Time: 1.6 s\n",
      "Epoch 001: Training Loss: 0.847, Validation Loss: 0.824, Time: 1.6 s\n",
      "Epoch 002: Training Loss: 0.791, Validation Loss: 0.754, Time: 1.5 s\n",
      "Epoch 003: Training Loss: 0.757, Validation Loss: 0.744, Time: 1.5 s\n",
      "Epoch 004: Training Loss: 0.736, Validation Loss: 0.732, Time: 1.5 s\n",
      "Epoch 005: Training Loss: 0.718, Validation Loss: 0.708, Time: 1.5 s\n",
      "Epoch 006: Training Loss: 0.707, Validation Loss: 0.705, Time: 1.4 s\n",
      "Epoch 007: Training Loss: 0.699, Validation Loss: 0.691, Time: 1.4 s\n",
      "Epoch 008: Training Loss: 0.691, Validation Loss: 0.689, Time: 1.4 s\n",
      "Epoch 009: Training Loss: 0.685, Validation Loss: 0.677, Time: 1.4 s\n",
      "Epoch 010: Training Loss: 0.682, Validation Loss: 0.675, Time: 1.4 s\n",
      "Epoch 011: Training Loss: 0.679, Validation Loss: 0.681, Time: 1.4 s\n",
      "Epoch 012: Training Loss: 0.675, Validation Loss: 0.673, Time: 1.4 s\n",
      "Epoch 013: Training Loss: 0.672, Validation Loss: 0.678, Time: 1.4 s\n",
      "Epoch 014: Training Loss: 0.669, Validation Loss: 0.668, Time: 1.4 s\n",
      "Epoch 015: Training Loss: 0.665, Validation Loss: 0.670, Time: 1.4 s\n",
      "Epoch 016: Training Loss: 0.664, Validation Loss: 0.663, Time: 1.4 s\n",
      "Epoch 017: Training Loss: 0.662, Validation Loss: 0.669, Time: 1.4 s\n",
      "Epoch 018: Training Loss: 0.659, Validation Loss: 0.663, Time: 1.4 s\n",
      "Epoch 019: Training Loss: 0.661, Validation Loss: 0.666, Time: 1.4 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 020: Training Loss: 0.656, Validation Loss: 0.665, Time: 1.5 s\n",
      "Epoch 021: Training Loss: 0.656, Validation Loss: 0.657, Time: 1.5 s\n",
      "Epoch 022: Training Loss: 0.656, Validation Loss: 0.658, Time: 1.4 s\n",
      "Epoch 023: Training Loss: 0.655, Validation Loss: 0.662, Time: 1.4 s\n",
      "Epoch 024: Training Loss: 0.653, Validation Loss: 0.665, Time: 1.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 025: Training Loss: 0.656, Validation Loss: 0.653, Time: 1.4 s\n",
      "Epoch 026: Training Loss: 0.656, Validation Loss: 0.665, Time: 1.5 s\n",
      "Epoch 027: Training Loss: 0.653, Validation Loss: 0.679, Time: 1.4 s\n",
      "Epoch 028: Training Loss: 0.653, Validation Loss: 0.659, Time: 1.4 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 029: Training Loss: 0.652, Validation Loss: 0.658, Time: 1.4 s\n",
      "Epoch 030: Training Loss: 0.652, Validation Loss: 0.657, Time: 1.4 s\n",
      "Epoch 031: Training Loss: 0.654, Validation Loss: 0.671, Time: 1.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 032: Training Loss: 0.653, Validation Loss: 0.653, Time: 1.4 s\n",
      "Epoch 033: Training Loss: 0.652, Validation Loss: 0.668, Time: 1.4 s\n",
      "Epoch 034: Training Loss: 0.652, Validation Loss: 0.666, Time: 1.4 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 50.37 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2499744   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2523304   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,545,880\n",
      "Trainable params: 5,545,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2495616   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,499,744\n",
      "Trainable params: 2,499,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2514984   \n",
      "=================================================================\n",
      "Total params: 2,523,304\n",
      "Trainable params: 2,523,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.871, Validation Cluster: 0.828, Validation AE: 0.735], Label Change: 0.061, Time: 11.8 s\n",
      "Iter 001 Loss: [Training: 0.967, Validation Cluster: 0.922, Validation AE: 0.735], Label Change: 0.012, Time: 11.6 s\n",
      "Iter 002 Loss: [Training: 0.984, Validation Cluster: 0.956, Validation AE: 0.737], Label Change: 0.012, Time: 12.1 s\n",
      "Iter 003 Loss: [Training: 0.982, Validation Cluster: 0.965, Validation AE: 0.738], Label Change: 0.004, Time: 11.9 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.978, Validation Cluster: 0.973, Validation AE: 0.735], Label Change: 0.003, Time: 12.1 s\n",
      "Iter 005 Loss: [Training: 0.976, Validation Cluster: 0.972, Validation AE: 0.736], Label Change: 0.002, Time: 12.4 s\n",
      "Iter 006 Loss: [Training: 0.976, Validation Cluster: 0.975, Validation AE: 0.740], Label Change: 0.002, Time: 12.0 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.73965806 not improving.\n",
      "Proportion of Labels Changed:  0.001528321711720317  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     2321\n",
      "1     2414\n",
      "2     1298\n",
      "3     1498\n",
      "4      695\n",
      "5      603\n",
      "6      762\n",
      "7      310\n",
      "8      224\n",
      "9      187\n",
      "10     157\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 154.3366141319275\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n",
      "Weight files for count models not detected. Training HVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.670, Validation Loss: 0.498, Time: 3.9 s\n",
      "Epoch 001: Training Loss: 0.492, Validation Loss: 0.480, Time: 3.9 s\n",
      "Epoch 002: Training Loss: 0.481, Validation Loss: 0.473, Time: 3.8 s\n",
      "Epoch 003: Training Loss: 0.477, Validation Loss: 0.470, Time: 3.9 s\n",
      "Epoch 004: Training Loss: 0.475, Validation Loss: 0.472, Time: 3.9 s\n",
      "Epoch 005: Training Loss: 0.474, Validation Loss: 0.469, Time: 3.9 s\n",
      "Epoch 006: Training Loss: 0.472, Validation Loss: 0.468, Time: 3.8 s\n",
      "Epoch 007: Training Loss: 0.472, Validation Loss: 0.470, Time: 3.8 s\n",
      "Epoch 008: Training Loss: 0.471, Validation Loss: 0.468, Time: 3.7 s\n",
      "Epoch 009: Training Loss: 0.470, Validation Loss: 0.467, Time: 3.7 s\n",
      "Epoch 010: Training Loss: 0.470, Validation Loss: 0.466, Time: 3.7 s\n",
      "Epoch 011: Training Loss: 0.469, Validation Loss: 0.463, Time: 3.8 s\n",
      "Epoch 012: Training Loss: 0.469, Validation Loss: 0.465, Time: 3.8 s\n",
      "Epoch 013: Training Loss: 0.468, Validation Loss: 0.465, Time: 3.7 s\n",
      "Epoch 014: Training Loss: 0.468, Validation Loss: 0.464, Time: 3.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 015: Training Loss: 0.467, Validation Loss: 0.464, Time: 3.7 s\n",
      "Epoch 016: Training Loss: 0.467, Validation Loss: 0.464, Time: 3.8 s\n",
      "Epoch 017: Training Loss: 0.467, Validation Loss: 0.465, Time: 3.7 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 018: Training Loss: 0.466, Validation Loss: 0.464, Time: 3.8 s\n",
      "Epoch 019: Training Loss: 0.466, Validation Loss: 0.465, Time: 3.8 s\n",
      "Epoch 020: Training Loss: 0.466, Validation Loss: 0.465, Time: 3.7 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 79.62 seconds\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Training LVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.417, Validation Loss: 0.385, Time: 33.0 s\n",
      "Epoch 001: Training Loss: 0.380, Validation Loss: 0.381, Time: 34.0 s\n",
      "Epoch 002: Training Loss: 0.378, Validation Loss: 0.383, Time: 33.1 s\n",
      "Epoch 003: Training Loss: 0.377, Validation Loss: 0.379, Time: 32.4 s\n",
      "Epoch 004: Training Loss: 0.376, Validation Loss: 0.379, Time: 32.4 s\n",
      "Epoch 005: Training Loss: 0.376, Validation Loss: 0.381, Time: 33.0 s\n",
      "Epoch 006: Training Loss: 0.375, Validation Loss: 0.379, Time: 35.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 007: Training Loss: 0.374, Validation Loss: 0.381, Time: 36.5 s\n",
      "Epoch 008: Training Loss: 0.373, Validation Loss: 0.380, Time: 34.3 s\n",
      "Epoch 009: Training Loss: 0.374, Validation Loss: 0.381, Time: 32.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 010: Training Loss: 0.373, Validation Loss: 0.379, Time: 35.2 s\n",
      "Epoch 011: Training Loss: 0.373, Validation Loss: 0.377, Time: 32.7 s\n",
      "Epoch 012: Training Loss: 0.373, Validation Loss: 0.378, Time: 33.6 s\n",
      "Epoch 013: Training Loss: 0.373, Validation Loss: 0.377, Time: 31.4 s\n",
      "Epoch 014: Training Loss: 0.373, Validation Loss: 0.379, Time: 30.1 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 015: Training Loss: 0.373, Validation Loss: 0.379, Time: 30.6 s\n",
      "Epoch 016: Training Loss: 0.373, Validation Loss: 0.380, Time: 31.0 s\n",
      "Epoch 017: Training Loss: 0.372, Validation Loss: 0.379, Time: 33.3 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-05\n",
      "Epoch 018: Training Loss: 0.373, Validation Loss: 0.381, Time: 32.5 s\n",
      "Epoch 019: Training Loss: 0.372, Validation Loss: 0.378, Time: 32.7 s\n",
      "Epoch 020: Training Loss: 0.372, Validation Loss: 0.379, Time: 34.4 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 695.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.903, Validation Loss: 0.819, Time: 3.3 s\n",
      "Epoch 001: Training Loss: 0.789, Validation Loss: 0.747, Time: 3.0 s\n",
      "Epoch 002: Training Loss: 0.743, Validation Loss: 0.717, Time: 3.0 s\n",
      "Epoch 003: Training Loss: 0.720, Validation Loss: 0.700, Time: 3.0 s\n",
      "Epoch 004: Training Loss: 0.706, Validation Loss: 0.688, Time: 2.9 s\n",
      "Epoch 005: Training Loss: 0.695, Validation Loss: 0.680, Time: 2.9 s\n",
      "Epoch 006: Training Loss: 0.688, Validation Loss: 0.677, Time: 2.9 s\n",
      "Epoch 007: Training Loss: 0.682, Validation Loss: 0.673, Time: 2.9 s\n",
      "Epoch 008: Training Loss: 0.679, Validation Loss: 0.670, Time: 2.9 s\n",
      "Epoch 009: Training Loss: 0.675, Validation Loss: 0.666, Time: 2.8 s\n",
      "Epoch 010: Training Loss: 0.672, Validation Loss: 0.666, Time: 2.9 s\n",
      "Epoch 011: Training Loss: 0.669, Validation Loss: 0.663, Time: 2.8 s\n",
      "Epoch 012: Training Loss: 0.667, Validation Loss: 0.662, Time: 2.8 s\n",
      "Epoch 013: Training Loss: 0.665, Validation Loss: 0.660, Time: 2.9 s\n",
      "Epoch 014: Training Loss: 0.663, Validation Loss: 0.657, Time: 2.8 s\n",
      "Epoch 015: Training Loss: 0.662, Validation Loss: 0.658, Time: 3.0 s\n",
      "Epoch 016: Training Loss: 0.660, Validation Loss: 0.658, Time: 3.0 s\n",
      "Epoch 017: Training Loss: 0.659, Validation Loss: 0.657, Time: 3.0 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 018: Training Loss: 0.657, Validation Loss: 0.655, Time: 2.9 s\n",
      "Epoch 019: Training Loss: 0.656, Validation Loss: 0.655, Time: 2.9 s\n",
      "Epoch 020: Training Loss: 0.656, Validation Loss: 0.655, Time: 2.9 s\n",
      "Epoch 021: Training Loss: 0.655, Validation Loss: 0.654, Time: 2.9 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 022: Training Loss: 0.655, Validation Loss: 0.653, Time: 2.9 s\n",
      "Epoch 023: Training Loss: 0.654, Validation Loss: 0.653, Time: 2.9 s\n",
      "Epoch 024: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.9 s\n",
      "Epoch 025: Training Loss: 0.654, Validation Loss: 0.654, Time: 2.9 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 026: Training Loss: 0.654, Validation Loss: 0.653, Time: 2.9 s\n",
      "Epoch 027: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.9 s\n",
      "Epoch 028: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.9 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 029: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.9 s\n",
      "Epoch 030: Training Loss: 0.654, Validation Loss: 0.654, Time: 2.9 s\n",
      "Epoch 031: Training Loss: 0.653, Validation Loss: 0.654, Time: 2.9 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 93.28 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502816   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526400   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,048\n",
      "Trainable params: 5,552,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498688   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,816\n",
      "Trainable params: 2,502,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518080   \n",
      "=================================================================\n",
      "Total params: 2,526,400\n",
      "Trainable params: 2,526,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.884, Validation Cluster: 0.848, Validation AE: 0.760], Label Change: 0.049, Time: 22.9 s\n",
      "Iter 001 Loss: [Training: 0.983, Validation Cluster: 0.950, Validation AE: 0.767], Label Change: 0.012, Time: 22.0 s\n",
      "Iter 002 Loss: [Training: 1.006, Validation Cluster: 0.984, Validation AE: 0.768], Label Change: 0.008, Time: 22.0 s\n",
      "Iter 003 Loss: [Training: 1.004, Validation Cluster: 0.996, Validation AE: 0.770], Label Change: 0.003, Time: 21.9 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.998, Validation Cluster: 1.006, Validation AE: 0.770], Label Change: 0.002, Time: 21.9 s\n",
      "Iter 005 Loss: [Training: 0.996, Validation Cluster: 1.004, Validation AE: 0.769], Label Change: 0.001, Time: 23.0 s\n",
      "Iter 006 Loss: [Training: 0.994, Validation Cluster: 1.003, Validation AE: 0.769], Label Change: 0.002, Time: 21.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.7693984 not improving.\n",
      "Proportion of Labels Changed:  0.002387888628874349  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     4786\n",
      "1     3582\n",
      "2     3098\n",
      "3     2692\n",
      "4     1266\n",
      "5     1746\n",
      "6     1334\n",
      "7      910\n",
      "8      707\n",
      "9      402\n",
      "10     416\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 311.1596791744232\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.903, Validation Loss: 0.819, Time: 3.2 s\n",
      "Epoch 001: Training Loss: 0.789, Validation Loss: 0.747, Time: 2.9 s\n",
      "Epoch 002: Training Loss: 0.743, Validation Loss: 0.717, Time: 3.0 s\n",
      "Epoch 003: Training Loss: 0.720, Validation Loss: 0.700, Time: 2.9 s\n",
      "Epoch 004: Training Loss: 0.706, Validation Loss: 0.688, Time: 2.8 s\n",
      "Epoch 005: Training Loss: 0.695, Validation Loss: 0.680, Time: 2.8 s\n",
      "Epoch 006: Training Loss: 0.688, Validation Loss: 0.677, Time: 2.8 s\n",
      "Epoch 007: Training Loss: 0.682, Validation Loss: 0.673, Time: 2.9 s\n",
      "Epoch 008: Training Loss: 0.679, Validation Loss: 0.670, Time: 2.8 s\n",
      "Epoch 009: Training Loss: 0.675, Validation Loss: 0.666, Time: 2.9 s\n",
      "Epoch 010: Training Loss: 0.672, Validation Loss: 0.666, Time: 2.9 s\n",
      "Epoch 011: Training Loss: 0.669, Validation Loss: 0.663, Time: 2.8 s\n",
      "Epoch 012: Training Loss: 0.667, Validation Loss: 0.662, Time: 2.8 s\n",
      "Epoch 013: Training Loss: 0.665, Validation Loss: 0.660, Time: 2.8 s\n",
      "Epoch 014: Training Loss: 0.663, Validation Loss: 0.657, Time: 2.8 s\n",
      "Epoch 015: Training Loss: 0.662, Validation Loss: 0.658, Time: 2.8 s\n",
      "Epoch 016: Training Loss: 0.660, Validation Loss: 0.658, Time: 2.8 s\n",
      "Epoch 017: Training Loss: 0.659, Validation Loss: 0.657, Time: 2.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 018: Training Loss: 0.657, Validation Loss: 0.655, Time: 2.8 s\n",
      "Epoch 019: Training Loss: 0.656, Validation Loss: 0.655, Time: 2.8 s\n",
      "Epoch 020: Training Loss: 0.656, Validation Loss: 0.655, Time: 2.8 s\n",
      "Epoch 021: Training Loss: 0.655, Validation Loss: 0.654, Time: 2.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 022: Training Loss: 0.655, Validation Loss: 0.653, Time: 2.9 s\n",
      "Epoch 023: Training Loss: 0.654, Validation Loss: 0.653, Time: 2.8 s\n",
      "Epoch 024: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.8 s\n",
      "Epoch 025: Training Loss: 0.654, Validation Loss: 0.654, Time: 2.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 026: Training Loss: 0.654, Validation Loss: 0.653, Time: 2.8 s\n",
      "Epoch 027: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.8 s\n",
      "Epoch 028: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 029: Training Loss: 0.654, Validation Loss: 0.655, Time: 2.8 s\n",
      "Epoch 030: Training Loss: 0.654, Validation Loss: 0.654, Time: 2.8 s\n",
      "Epoch 031: Training Loss: 0.653, Validation Loss: 0.654, Time: 2.8 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 90.91 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502816   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526400   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,048\n",
      "Trainable params: 5,552,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498688   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,816\n",
      "Trainable params: 2,502,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518080   \n",
      "=================================================================\n",
      "Total params: 2,526,400\n",
      "Trainable params: 2,526,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.882, Validation Cluster: 0.847, Validation AE: 0.760], Label Change: 0.050, Time: 24.2 s\n",
      "Iter 001 Loss: [Training: 0.982, Validation Cluster: 0.949, Validation AE: 0.766], Label Change: 0.012, Time: 21.7 s\n",
      "Iter 002 Loss: [Training: 1.004, Validation Cluster: 0.984, Validation AE: 0.768], Label Change: 0.008, Time: 21.0 s\n",
      "Iter 003 Loss: [Training: 1.002, Validation Cluster: 0.994, Validation AE: 0.770], Label Change: 0.003, Time: 21.5 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.997, Validation Cluster: 1.005, Validation AE: 0.771], Label Change: 0.002, Time: 21.7 s\n",
      "Iter 005 Loss: [Training: 0.995, Validation Cluster: 1.002, Validation AE: 0.769], Label Change: 0.001, Time: 21.8 s\n",
      "Iter 006 Loss: [Training: 0.992, Validation Cluster: 1.002, Validation AE: 0.769], Label Change: 0.002, Time: 21.3 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.76935476 not improving.\n",
      "Proportion of Labels Changed:  0.0024356464014518363  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     4790\n",
      "1     3577\n",
      "2     3098\n",
      "3     2686\n",
      "4     1267\n",
      "5     1754\n",
      "6     1333\n",
      "7      911\n",
      "8      706\n",
      "9      401\n",
      "10     416\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 302.3038241863251\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n",
      "Weight files for count models not detected. Training HVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.469, Validation Loss: 0.441, Time: 7.3 s\n",
      "Epoch 001: Training Loss: 0.441, Validation Loss: 0.439, Time: 7.3 s\n",
      "Epoch 002: Training Loss: 0.439, Validation Loss: 0.437, Time: 7.5 s\n",
      "Epoch 003: Training Loss: 0.438, Validation Loss: 0.437, Time: 7.1 s\n",
      "Epoch 004: Training Loss: 0.437, Validation Loss: 0.436, Time: 6.8 s\n",
      "Epoch 005: Training Loss: 0.437, Validation Loss: 0.435, Time: 6.7 s\n",
      "Epoch 006: Training Loss: 0.436, Validation Loss: 0.435, Time: 6.7 s\n",
      "Epoch 007: Training Loss: 0.436, Validation Loss: 0.435, Time: 6.9 s\n",
      "Epoch 008: Training Loss: 0.436, Validation Loss: 0.435, Time: 7.1 s\n",
      "Epoch 009: Training Loss: 0.435, Validation Loss: 0.435, Time: 7.3 s\n",
      "Epoch 010: Training Loss: 0.435, Validation Loss: 0.435, Time: 7.2 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 011: Training Loss: 0.434, Validation Loss: 0.434, Time: 7.1 s\n",
      "Epoch 012: Training Loss: 0.434, Validation Loss: 0.434, Time: 6.8 s\n",
      "Epoch 013: Training Loss: 0.434, Validation Loss: 0.434, Time: 7.0 s\n",
      "Epoch 014: Training Loss: 0.434, Validation Loss: 0.433, Time: 6.6 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 015: Training Loss: 0.433, Validation Loss: 0.434, Time: 6.7 s\n",
      "Epoch 016: Training Loss: 0.433, Validation Loss: 0.434, Time: 6.7 s\n",
      "Epoch 017: Training Loss: 0.433, Validation Loss: 0.434, Time: 6.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 018: Training Loss: 0.433, Validation Loss: 0.433, Time: 6.7 s\n",
      "Epoch 019: Training Loss: 0.433, Validation Loss: 0.434, Time: 7.0 s\n",
      "Epoch 020: Training Loss: 0.433, Validation Loss: 0.433, Time: 6.9 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 146.32 seconds\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Training LVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.400, Validation Loss: 0.381, Time: 50.7 s\n",
      "Epoch 001: Training Loss: 0.380, Validation Loss: 0.379, Time: 50.6 s\n",
      "Epoch 002: Training Loss: 0.378, Validation Loss: 0.379, Time: 47.2 s\n",
      "Epoch 003: Training Loss: 0.378, Validation Loss: 0.378, Time: 46.2 s\n",
      "Epoch 004: Training Loss: 0.377, Validation Loss: 0.378, Time: 50.3 s\n",
      "Epoch 005: Training Loss: 0.377, Validation Loss: 0.377, Time: 46.9 s\n",
      "Epoch 006: Training Loss: 0.376, Validation Loss: 0.377, Time: 47.5 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 007: Training Loss: 0.375, Validation Loss: 0.377, Time: 47.0 s\n",
      "Epoch 008: Training Loss: 0.375, Validation Loss: 0.377, Time: 46.8 s\n",
      "Epoch 009: Training Loss: 0.375, Validation Loss: 0.377, Time: 48.0 s\n",
      "Epoch 010: Training Loss: 0.375, Validation Loss: 0.377, Time: 50.1 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 011: Training Loss: 0.374, Validation Loss: 0.376, Time: 48.1 s\n",
      "Epoch 012: Training Loss: 0.374, Validation Loss: 0.376, Time: 51.8 s\n",
      "Epoch 013: Training Loss: 0.374, Validation Loss: 0.376, Time: 51.2 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 014: Training Loss: 0.374, Validation Loss: 0.377, Time: 50.0 s\n",
      "Epoch 015: Training Loss: 0.374, Validation Loss: 0.376, Time: 51.3 s\n",
      "Epoch 016: Training Loss: 0.374, Validation Loss: 0.376, Time: 50.4 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 834.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.850, Validation Loss: 0.802, Time: 6.3 s\n",
      "Epoch 001: Training Loss: 0.741, Validation Loss: 0.754, Time: 6.0 s\n",
      "Epoch 002: Training Loss: 0.710, Validation Loss: 0.732, Time: 5.8 s\n",
      "Epoch 003: Training Loss: 0.693, Validation Loss: 0.718, Time: 5.8 s\n",
      "Epoch 004: Training Loss: 0.685, Validation Loss: 0.714, Time: 5.8 s\n",
      "Epoch 005: Training Loss: 0.679, Validation Loss: 0.708, Time: 6.0 s\n",
      "Epoch 006: Training Loss: 0.674, Validation Loss: 0.709, Time: 6.0 s\n",
      "Epoch 007: Training Loss: 0.670, Validation Loss: 0.700, Time: 5.8 s\n",
      "Epoch 008: Training Loss: 0.667, Validation Loss: 0.701, Time: 5.9 s\n",
      "Epoch 009: Training Loss: 0.664, Validation Loss: 0.695, Time: 5.8 s\n",
      "Epoch 010: Training Loss: 0.662, Validation Loss: 0.695, Time: 5.8 s\n",
      "Epoch 011: Training Loss: 0.660, Validation Loss: 0.692, Time: 5.9 s\n",
      "Epoch 012: Training Loss: 0.658, Validation Loss: 0.690, Time: 6.0 s\n",
      "Epoch 013: Training Loss: 0.656, Validation Loss: 0.689, Time: 5.9 s\n",
      "Epoch 014: Training Loss: 0.654, Validation Loss: 0.686, Time: 5.9 s\n",
      "Epoch 015: Training Loss: 0.652, Validation Loss: 0.685, Time: 5.9 s\n",
      "Epoch 016: Training Loss: 0.651, Validation Loss: 0.684, Time: 5.9 s\n",
      "Epoch 017: Training Loss: 0.649, Validation Loss: 0.682, Time: 5.8 s\n",
      "Epoch 018: Training Loss: 0.648, Validation Loss: 0.682, Time: 5.8 s\n",
      "Epoch 019: Training Loss: 0.647, Validation Loss: 0.681, Time: 5.8 s\n",
      "Epoch 020: Training Loss: 0.645, Validation Loss: 0.684, Time: 5.8 s\n",
      "Epoch 021: Training Loss: 0.644, Validation Loss: 0.678, Time: 5.8 s\n",
      "Epoch 022: Training Loss: 0.643, Validation Loss: 0.676, Time: 5.8 s\n",
      "Epoch 023: Training Loss: 0.642, Validation Loss: 0.677, Time: 5.8 s\n",
      "Epoch 024: Training Loss: 0.641, Validation Loss: 0.674, Time: 5.8 s\n",
      "Epoch 025: Training Loss: 0.640, Validation Loss: 0.674, Time: 5.8 s\n",
      "Epoch 026: Training Loss: 0.639, Validation Loss: 0.673, Time: 6.0 s\n",
      "Epoch 027: Training Loss: 0.638, Validation Loss: 0.673, Time: 6.0 s\n",
      "Epoch 028: Training Loss: 0.637, Validation Loss: 0.671, Time: 5.9 s\n",
      "Epoch 029: Training Loss: 0.636, Validation Loss: 0.670, Time: 5.9 s\n",
      "Epoch 030: Training Loss: 0.635, Validation Loss: 0.670, Time: 5.8 s\n",
      "Epoch 031: Training Loss: 0.634, Validation Loss: 0.669, Time: 5.9 s\n",
      "Epoch 032: Training Loss: 0.633, Validation Loss: 0.668, Time: 5.9 s\n",
      "Epoch 033: Training Loss: 0.632, Validation Loss: 0.667, Time: 5.9 s\n",
      "Epoch 034: Training Loss: 0.632, Validation Loss: 0.667, Time: 5.8 s\n",
      "Epoch 035: Training Loss: 0.631, Validation Loss: 0.669, Time: 5.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 036: Training Loss: 0.630, Validation Loss: 0.665, Time: 5.8 s\n",
      "Epoch 037: Training Loss: 0.629, Validation Loss: 0.666, Time: 5.8 s\n",
      "Epoch 038: Training Loss: 0.629, Validation Loss: 0.666, Time: 5.8 s\n",
      "Epoch 039: Training Loss: 0.628, Validation Loss: 0.666, Time: 5.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 040: Training Loss: 0.628, Validation Loss: 0.667, Time: 5.8 s\n",
      "Epoch 041: Training Loss: 0.628, Validation Loss: 0.667, Time: 5.8 s\n",
      "Epoch 042: Training Loss: 0.628, Validation Loss: 0.667, Time: 5.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 043: Training Loss: 0.628, Validation Loss: 0.664, Time: 5.8 s\n",
      "Epoch 044: Training Loss: 0.628, Validation Loss: 0.664, Time: 6.0 s\n",
      "Epoch 045: Training Loss: 0.628, Validation Loss: 0.669, Time: 6.2 s\n",
      "Epoch 046: Training Loss: 0.627, Validation Loss: 0.665, Time: 6.0 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 047: Training Loss: 0.627, Validation Loss: 0.666, Time: 6.0 s\n",
      "Epoch 048: Training Loss: 0.627, Validation Loss: 0.666, Time: 6.3 s\n",
      "Epoch 049: Training Loss: 0.627, Validation Loss: 0.664, Time: 6.2 s\n",
      "\n",
      "Decaying Learning Rate to: 4.1152268e-07\n",
      "Epoch 050: Training Loss: 0.627, Validation Loss: 0.665, Time: 6.1 s\n",
      "Epoch 051: Training Loss: 0.627, Validation Loss: 0.668, Time: 6.1 s\n",
      "Epoch 052: Training Loss: 0.627, Validation Loss: 0.666, Time: 6.1 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 313.46 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  320       \n",
      "=================================================================\n",
      "Total params: 5,552,273\n",
      "Trainable params: 5,552,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.829, Validation Cluster: 0.820, Validation AE: 0.781], Label Change: 0.107, Time: 73.1 s\n",
      "Iter 001 Loss: [Training: 0.921, Validation Cluster: 0.909, Validation AE: 0.795], Label Change: 0.062, Time: 48.4 s\n",
      "Iter 002 Loss: [Training: 0.960, Validation Cluster: 0.956, Validation AE: 0.800], Label Change: 0.037, Time: 44.6 s\n",
      "Iter 003 Loss: [Training: 0.967, Validation Cluster: 0.971, Validation AE: 0.802], Label Change: 0.015, Time: 45.5 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.970, Validation Cluster: 0.987, Validation AE: 0.805], Label Change: 0.006, Time: 46.1 s\n",
      "Iter 005 Loss: [Training: 0.969, Validation Cluster: 0.988, Validation AE: 0.805], Label Change: 0.009, Time: 44.0 s\n",
      "Iter 006 Loss: [Training: 0.967, Validation Cluster: 0.984, Validation AE: 0.803], Label Change: 0.006, Time: 43.9 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Iter 007 Loss: [Training: 0.967, Validation Cluster: 0.989, Validation AE: 0.805], Label Change: 0.004, Time: 44.8 s\n",
      "\n",
      "Autoencoder_loss  0.8050316 not improving.\n",
      "Proportion of Labels Changed:  0.00444147284970629  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0    12262\n",
      "1     4075\n",
      "2     6404\n",
      "3     3491\n",
      "4     3682\n",
      "5     1842\n",
      "6     1262\n",
      "7     5247\n",
      "8      825\n",
      "9     2788\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 792.7320499420166\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.850, Validation Loss: 0.802, Time: 6.7 s\n",
      "Epoch 001: Training Loss: 0.741, Validation Loss: 0.754, Time: 6.1 s\n",
      "Epoch 002: Training Loss: 0.710, Validation Loss: 0.732, Time: 6.1 s\n",
      "Epoch 003: Training Loss: 0.693, Validation Loss: 0.718, Time: 6.0 s\n",
      "Epoch 004: Training Loss: 0.685, Validation Loss: 0.714, Time: 5.8 s\n",
      "Epoch 005: Training Loss: 0.679, Validation Loss: 0.708, Time: 5.8 s\n",
      "Epoch 006: Training Loss: 0.674, Validation Loss: 0.709, Time: 5.8 s\n",
      "Epoch 007: Training Loss: 0.670, Validation Loss: 0.700, Time: 5.9 s\n",
      "Epoch 008: Training Loss: 0.667, Validation Loss: 0.701, Time: 5.8 s\n",
      "Epoch 009: Training Loss: 0.664, Validation Loss: 0.695, Time: 5.8 s\n",
      "Epoch 010: Training Loss: 0.662, Validation Loss: 0.695, Time: 5.8 s\n",
      "Epoch 011: Training Loss: 0.660, Validation Loss: 0.692, Time: 5.8 s\n",
      "Epoch 012: Training Loss: 0.658, Validation Loss: 0.690, Time: 5.8 s\n",
      "Epoch 013: Training Loss: 0.656, Validation Loss: 0.689, Time: 5.8 s\n",
      "Epoch 014: Training Loss: 0.654, Validation Loss: 0.686, Time: 5.8 s\n",
      "Epoch 015: Training Loss: 0.652, Validation Loss: 0.685, Time: 5.8 s\n",
      "Epoch 016: Training Loss: 0.651, Validation Loss: 0.684, Time: 5.8 s\n",
      "Epoch 017: Training Loss: 0.649, Validation Loss: 0.682, Time: 5.8 s\n",
      "Epoch 018: Training Loss: 0.648, Validation Loss: 0.682, Time: 5.8 s\n",
      "Epoch 019: Training Loss: 0.647, Validation Loss: 0.681, Time: 5.8 s\n",
      "Epoch 020: Training Loss: 0.645, Validation Loss: 0.684, Time: 5.9 s\n",
      "Epoch 021: Training Loss: 0.644, Validation Loss: 0.678, Time: 6.0 s\n",
      "Epoch 022: Training Loss: 0.643, Validation Loss: 0.676, Time: 6.0 s\n",
      "Epoch 023: Training Loss: 0.642, Validation Loss: 0.677, Time: 5.8 s\n",
      "Epoch 024: Training Loss: 0.641, Validation Loss: 0.674, Time: 5.8 s\n",
      "Epoch 025: Training Loss: 0.640, Validation Loss: 0.674, Time: 5.7 s\n",
      "Epoch 026: Training Loss: 0.639, Validation Loss: 0.673, Time: 5.8 s\n",
      "Epoch 027: Training Loss: 0.638, Validation Loss: 0.673, Time: 5.8 s\n",
      "Epoch 028: Training Loss: 0.637, Validation Loss: 0.671, Time: 5.8 s\n",
      "Epoch 029: Training Loss: 0.636, Validation Loss: 0.670, Time: 5.8 s\n",
      "Epoch 030: Training Loss: 0.635, Validation Loss: 0.670, Time: 5.8 s\n",
      "Epoch 031: Training Loss: 0.634, Validation Loss: 0.669, Time: 5.8 s\n",
      "Epoch 032: Training Loss: 0.633, Validation Loss: 0.668, Time: 5.8 s\n",
      "Epoch 033: Training Loss: 0.632, Validation Loss: 0.667, Time: 5.8 s\n",
      "Epoch 034: Training Loss: 0.632, Validation Loss: 0.667, Time: 5.8 s\n",
      "Epoch 035: Training Loss: 0.631, Validation Loss: 0.669, Time: 5.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 036: Training Loss: 0.630, Validation Loss: 0.665, Time: 5.7 s\n",
      "Epoch 037: Training Loss: 0.629, Validation Loss: 0.666, Time: 5.8 s\n",
      "Epoch 038: Training Loss: 0.629, Validation Loss: 0.666, Time: 5.9 s\n",
      "Epoch 039: Training Loss: 0.628, Validation Loss: 0.666, Time: 5.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 040: Training Loss: 0.628, Validation Loss: 0.667, Time: 5.8 s\n",
      "Epoch 041: Training Loss: 0.628, Validation Loss: 0.667, Time: 6.0 s\n",
      "Epoch 042: Training Loss: 0.628, Validation Loss: 0.667, Time: 5.9 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 043: Training Loss: 0.628, Validation Loss: 0.664, Time: 5.8 s\n",
      "Epoch 044: Training Loss: 0.628, Validation Loss: 0.664, Time: 5.8 s\n",
      "Epoch 045: Training Loss: 0.628, Validation Loss: 0.669, Time: 5.8 s\n",
      "Epoch 046: Training Loss: 0.627, Validation Loss: 0.665, Time: 5.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 047: Training Loss: 0.627, Validation Loss: 0.666, Time: 5.8 s\n",
      "Epoch 048: Training Loss: 0.627, Validation Loss: 0.666, Time: 5.8 s\n",
      "Epoch 049: Training Loss: 0.627, Validation Loss: 0.664, Time: 5.8 s\n",
      "\n",
      "Decaying Learning Rate to: 4.1152268e-07\n",
      "Epoch 050: Training Loss: 0.627, Validation Loss: 0.665, Time: 5.8 s\n",
      "Epoch 051: Training Loss: 0.627, Validation Loss: 0.668, Time: 5.8 s\n",
      "Epoch 052: Training Loss: 0.627, Validation Loss: 0.666, Time: 5.8 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 309.93 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,305\n",
      "Trainable params: 5,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.832, Validation Cluster: 0.823, Validation AE: 0.782], Label Change: 0.121, Time: 77.2 s\n",
      "Iter 001 Loss: [Training: 0.928, Validation Cluster: 0.916, Validation AE: 0.795], Label Change: 0.060, Time: 45.6 s\n",
      "Iter 002 Loss: [Training: 0.971, Validation Cluster: 0.967, Validation AE: 0.800], Label Change: 0.026, Time: 45.3 s\n",
      "Iter 003 Loss: [Training: 0.978, Validation Cluster: 0.981, Validation AE: 0.802], Label Change: 0.014, Time: 44.7 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.978, Validation Cluster: 0.994, Validation AE: 0.804], Label Change: 0.007, Time: 46.3 s\n",
      "Iter 005 Loss: [Training: 0.977, Validation Cluster: 0.995, Validation AE: 0.804], Label Change: 0.009, Time: 45.9 s\n",
      "Iter 006 Loss: [Training: 0.975, Validation Cluster: 0.991, Validation AE: 0.802], Label Change: 0.006, Time: 45.5 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Iter 007 Loss: [Training: 0.974, Validation Cluster: 0.995, Validation AE: 0.804], Label Change: 0.005, Time: 45.0 s\n",
      "\n",
      "Autoencoder_loss  0.8042762 not improving.\n",
      "Proportion of Labels Changed:  0.004632503940016237  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     11818\n",
      "1      3924\n",
      "2      6402\n",
      "3      3491\n",
      "4      3669\n",
      "5      1838\n",
      "6      1265\n",
      "7      5392\n",
      "8       826\n",
      "9      2965\n",
      "10      288\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 793.4828889369965\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n",
      "Weight files for count models not detected. Training HVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.435, Validation Loss: 0.424, Time: 13.6 s\n",
      "Epoch 001: Training Loss: 0.418, Validation Loss: 0.422, Time: 13.2 s\n",
      "Epoch 002: Training Loss: 0.415, Validation Loss: 0.419, Time: 13.0 s\n",
      "Epoch 003: Training Loss: 0.414, Validation Loss: 0.418, Time: 13.1 s\n",
      "Epoch 004: Training Loss: 0.412, Validation Loss: 0.417, Time: 13.0 s\n",
      "Epoch 005: Training Loss: 0.411, Validation Loss: 0.416, Time: 13.1 s\n",
      "Epoch 006: Training Loss: 0.411, Validation Loss: 0.415, Time: 13.5 s\n",
      "Epoch 007: Training Loss: 0.410, Validation Loss: 0.415, Time: 12.9 s\n",
      "Epoch 008: Training Loss: 0.410, Validation Loss: 0.415, Time: 13.0 s\n",
      "Epoch 009: Training Loss: 0.409, Validation Loss: 0.414, Time: 12.9 s\n",
      "Epoch 010: Training Loss: 0.409, Validation Loss: 0.414, Time: 12.9 s\n",
      "Epoch 011: Training Loss: 0.408, Validation Loss: 0.413, Time: 12.9 s\n",
      "Epoch 012: Training Loss: 0.408, Validation Loss: 0.413, Time: 12.9 s\n",
      "Epoch 013: Training Loss: 0.408, Validation Loss: 0.413, Time: 12.9 s\n",
      "Epoch 014: Training Loss: 0.407, Validation Loss: 0.412, Time: 12.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 015: Training Loss: 0.406, Validation Loss: 0.412, Time: 13.5 s\n",
      "Epoch 016: Training Loss: 0.406, Validation Loss: 0.412, Time: 12.9 s\n",
      "Epoch 017: Training Loss: 0.406, Validation Loss: 0.412, Time: 12.9 s\n",
      "Epoch 018: Training Loss: 0.406, Validation Loss: 0.412, Time: 12.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 019: Training Loss: 0.406, Validation Loss: 0.412, Time: 12.9 s\n",
      "Epoch 020: Training Loss: 0.406, Validation Loss: 0.412, Time: 12.9 s\n",
      "Epoch 021: Training Loss: 0.406, Validation Loss: 0.412, Time: 13.0 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 022: Training Loss: 0.406, Validation Loss: 0.411, Time: 12.9 s\n",
      "Epoch 023: Training Loss: 0.406, Validation Loss: 0.412, Time: 12.9 s\n",
      "Epoch 024: Training Loss: 0.406, Validation Loss: 0.412, Time: 13.3 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 326.14 seconds\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Training LVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.392, Validation Loss: 0.382, Time: 92.3 s\n",
      "Epoch 001: Training Loss: 0.380, Validation Loss: 0.381, Time: 88.4 s\n",
      "Epoch 002: Training Loss: 0.378, Validation Loss: 0.380, Time: 90.7 s\n",
      "Epoch 003: Training Loss: 0.378, Validation Loss: 0.379, Time: 101.8 s\n",
      "Epoch 004: Training Loss: 0.377, Validation Loss: 0.379, Time: 100.3 s\n",
      "Epoch 005: Training Loss: 0.377, Validation Loss: 0.379, Time: 91.5 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 006: Training Loss: 0.376, Validation Loss: 0.379, Time: 88.1 s\n",
      "Epoch 007: Training Loss: 0.376, Validation Loss: 0.379, Time: 97.1 s\n",
      "Epoch 008: Training Loss: 0.376, Validation Loss: 0.379, Time: 102.1 s\n",
      "Epoch 009: Training Loss: 0.376, Validation Loss: 0.379, Time: 103.8 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 010: Training Loss: 0.376, Validation Loss: 0.378, Time: 103.8 s\n",
      "Epoch 011: Training Loss: 0.376, Validation Loss: 0.378, Time: 102.5 s\n",
      "Epoch 012: Training Loss: 0.375, Validation Loss: 0.378, Time: 99.4 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 013: Training Loss: 0.375, Validation Loss: 0.378, Time: 97.9 s\n",
      "Epoch 014: Training Loss: 0.375, Validation Loss: 0.378, Time: 98.8 s\n",
      "Epoch 015: Training Loss: 0.375, Validation Loss: 0.378, Time: 96.0 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 1554.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.824, Validation Loss: 0.752, Time: 9.7 s\n",
      "Epoch 001: Training Loss: 0.721, Validation Loss: 0.721, Time: 8.9 s\n",
      "Epoch 002: Training Loss: 0.701, Validation Loss: 0.706, Time: 8.8 s\n",
      "Epoch 003: Training Loss: 0.691, Validation Loss: 0.707, Time: 8.9 s\n",
      "Epoch 004: Training Loss: 0.684, Validation Loss: 0.692, Time: 9.1 s\n",
      "Epoch 005: Training Loss: 0.679, Validation Loss: 0.686, Time: 8.8 s\n",
      "Epoch 006: Training Loss: 0.675, Validation Loss: 0.686, Time: 8.8 s\n",
      "Epoch 007: Training Loss: 0.672, Validation Loss: 0.685, Time: 8.8 s\n",
      "Epoch 008: Training Loss: 0.668, Validation Loss: 0.678, Time: 8.9 s\n",
      "Epoch 009: Training Loss: 0.666, Validation Loss: 0.676, Time: 8.8 s\n",
      "Epoch 010: Training Loss: 0.664, Validation Loss: 0.674, Time: 8.8 s\n",
      "Epoch 011: Training Loss: 0.662, Validation Loss: 0.677, Time: 8.8 s\n",
      "Epoch 012: Training Loss: 0.660, Validation Loss: 0.673, Time: 8.7 s\n",
      "Epoch 013: Training Loss: 0.658, Validation Loss: 0.670, Time: 8.7 s\n",
      "Epoch 014: Training Loss: 0.656, Validation Loss: 0.670, Time: 8.7 s\n",
      "Epoch 015: Training Loss: 0.654, Validation Loss: 0.671, Time: 8.8 s\n",
      "Epoch 016: Training Loss: 0.653, Validation Loss: 0.663, Time: 8.8 s\n",
      "Epoch 017: Training Loss: 0.651, Validation Loss: 0.667, Time: 9.0 s\n",
      "Epoch 018: Training Loss: 0.649, Validation Loss: 0.665, Time: 8.8 s\n",
      "Epoch 019: Training Loss: 0.648, Validation Loss: 0.663, Time: 8.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 020: Training Loss: 0.646, Validation Loss: 0.658, Time: 8.7 s\n",
      "Epoch 021: Training Loss: 0.645, Validation Loss: 0.657, Time: 8.7 s\n",
      "Epoch 022: Training Loss: 0.645, Validation Loss: 0.661, Time: 8.8 s\n",
      "Epoch 023: Training Loss: 0.645, Validation Loss: 0.658, Time: 8.8 s\n",
      "Epoch 024: Training Loss: 0.644, Validation Loss: 0.662, Time: 8.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 025: Training Loss: 0.643, Validation Loss: 0.658, Time: 8.7 s\n",
      "Epoch 026: Training Loss: 0.643, Validation Loss: 0.657, Time: 8.8 s\n",
      "Epoch 027: Training Loss: 0.643, Validation Loss: 0.659, Time: 8.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 028: Training Loss: 0.643, Validation Loss: 0.657, Time: 8.7 s\n",
      "Epoch 029: Training Loss: 0.643, Validation Loss: 0.658, Time: 8.8 s\n",
      "Epoch 030: Training Loss: 0.643, Validation Loss: 0.660, Time: 8.8 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 273.87 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,305\n",
      "Trainable params: 5,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.840, Validation Cluster: 0.821, Validation AE: 0.784], Label Change: 0.094, Time: 201.7 s\n",
      "Iter 001 Loss: [Training: 0.924, Validation Cluster: 0.902, Validation AE: 0.793], Label Change: 0.062, Time: 177.7 s\n",
      "Iter 002 Loss: [Training: 0.964, Validation Cluster: 0.950, Validation AE: 0.799], Label Change: 0.027, Time: 153.5 s\n",
      "Iter 003 Loss: [Training: 0.974, Validation Cluster: 0.963, Validation AE: 0.801], Label Change: 0.018, Time: 158.8 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.977, Validation Cluster: 0.978, Validation AE: 0.803], Label Change: 0.009, Time: 172.9 s\n",
      "Iter 005 Loss: [Training: 0.978, Validation Cluster: 0.976, Validation AE: 0.801], Label Change: 0.008, Time: 165.9 s\n",
      "Iter 006 Loss: [Training: 0.976, Validation Cluster: 0.976, Validation AE: 0.802], Label Change: 0.005, Time: 166.7 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Iter 007 Loss: [Training: 0.976, Validation Cluster: 0.984, Validation AE: 0.806], Label Change: 0.004, Time: 170.3 s\n",
      "\n",
      "Autoencoder_loss  0.80641484 not improving.\n",
      "Proportion of Labels Changed:  0.003900280183392766  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     15480\n",
      "1     17947\n",
      "2      3995\n",
      "3      4169\n",
      "4      2765\n",
      "5      2702\n",
      "6      6776\n",
      "7      1252\n",
      "8      1249\n",
      "9      2397\n",
      "10     4084\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 2086.3315110206604\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.824, Validation Loss: 0.752, Time: 11.4 s\n",
      "Epoch 001: Training Loss: 0.721, Validation Loss: 0.721, Time: 8.8 s\n",
      "Epoch 002: Training Loss: 0.701, Validation Loss: 0.706, Time: 8.6 s\n",
      "Epoch 003: Training Loss: 0.691, Validation Loss: 0.707, Time: 8.6 s\n",
      "Epoch 004: Training Loss: 0.684, Validation Loss: 0.692, Time: 8.7 s\n",
      "Epoch 005: Training Loss: 0.679, Validation Loss: 0.686, Time: 8.6 s\n",
      "Epoch 006: Training Loss: 0.675, Validation Loss: 0.686, Time: 8.6 s\n",
      "Epoch 007: Training Loss: 0.672, Validation Loss: 0.685, Time: 8.6 s\n",
      "Epoch 008: Training Loss: 0.668, Validation Loss: 0.678, Time: 8.6 s\n",
      "Epoch 009: Training Loss: 0.666, Validation Loss: 0.676, Time: 8.6 s\n",
      "Epoch 010: Training Loss: 0.664, Validation Loss: 0.674, Time: 8.7 s\n",
      "Epoch 011: Training Loss: 0.662, Validation Loss: 0.677, Time: 8.8 s\n",
      "Epoch 012: Training Loss: 0.660, Validation Loss: 0.673, Time: 8.7 s\n",
      "Epoch 013: Training Loss: 0.658, Validation Loss: 0.670, Time: 8.7 s\n",
      "Epoch 014: Training Loss: 0.656, Validation Loss: 0.670, Time: 8.6 s\n",
      "Epoch 015: Training Loss: 0.654, Validation Loss: 0.671, Time: 8.6 s\n",
      "Epoch 016: Training Loss: 0.653, Validation Loss: 0.663, Time: 8.6 s\n",
      "Epoch 017: Training Loss: 0.651, Validation Loss: 0.667, Time: 8.6 s\n",
      "Epoch 018: Training Loss: 0.649, Validation Loss: 0.665, Time: 8.6 s\n",
      "Epoch 019: Training Loss: 0.648, Validation Loss: 0.663, Time: 8.5 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 020: Training Loss: 0.646, Validation Loss: 0.658, Time: 8.6 s\n",
      "Epoch 021: Training Loss: 0.645, Validation Loss: 0.657, Time: 8.5 s\n",
      "Epoch 022: Training Loss: 0.645, Validation Loss: 0.661, Time: 8.5 s\n",
      "Epoch 023: Training Loss: 0.645, Validation Loss: 0.658, Time: 8.5 s\n",
      "Epoch 024: Training Loss: 0.644, Validation Loss: 0.662, Time: 8.8 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 025: Training Loss: 0.643, Validation Loss: 0.658, Time: 8.7 s\n",
      "Epoch 026: Training Loss: 0.643, Validation Loss: 0.657, Time: 8.6 s\n",
      "Epoch 027: Training Loss: 0.643, Validation Loss: 0.659, Time: 8.6 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 028: Training Loss: 0.643, Validation Loss: 0.657, Time: 8.6 s\n",
      "Epoch 029: Training Loss: 0.643, Validation Loss: 0.658, Time: 8.6 s\n",
      "Epoch 030: Training Loss: 0.643, Validation Loss: 0.660, Time: 8.5 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 270.15 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,305\n",
      "Trainable params: 5,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.840, Validation Cluster: 0.821, Validation AE: 0.784], Label Change: 0.094, Time: 202.9 s\n",
      "Iter 001 Loss: [Training: 0.924, Validation Cluster: 0.902, Validation AE: 0.793], Label Change: 0.062, Time: 174.2 s\n",
      "Iter 002 Loss: [Training: 0.964, Validation Cluster: 0.950, Validation AE: 0.799], Label Change: 0.027, Time: 174.0 s\n",
      "Iter 003 Loss: [Training: 0.974, Validation Cluster: 0.963, Validation AE: 0.801], Label Change: 0.018, Time: 179.6 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.977, Validation Cluster: 0.978, Validation AE: 0.803], Label Change: 0.009, Time: 134.1 s\n",
      "Iter 005 Loss: [Training: 0.978, Validation Cluster: 0.976, Validation AE: 0.801], Label Change: 0.008, Time: 199.3 s\n",
      "Iter 006 Loss: [Training: 0.976, Validation Cluster: 0.976, Validation AE: 0.802], Label Change: 0.005, Time: 194.2 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Iter 007 Loss: [Training: 0.976, Validation Cluster: 0.984, Validation AE: 0.806], Label Change: 0.004, Time: 194.0 s\n",
      "\n",
      "Autoencoder_loss  0.8063972 not improving.\n",
      "Proportion of Labels Changed:  0.003948038716250637  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     15472\n",
      "1     17945\n",
      "2      3995\n",
      "3      4169\n",
      "4      2765\n",
      "5      2694\n",
      "6      6779\n",
      "7      1252\n",
      "8      1248\n",
      "9      2411\n",
      "10     4086\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 2197.0594959259033\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n",
      "Weight files for count models not detected. Training HVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.415, Validation Loss: 0.406, Time: 22.4 s\n",
      "Epoch 001: Training Loss: 0.402, Validation Loss: 0.403, Time: 20.3 s\n",
      "Epoch 002: Training Loss: 0.399, Validation Loss: 0.401, Time: 20.3 s\n",
      "Epoch 003: Training Loss: 0.398, Validation Loss: 0.400, Time: 20.3 s\n",
      "Epoch 004: Training Loss: 0.397, Validation Loss: 0.399, Time: 20.7 s\n",
      "Epoch 005: Training Loss: 0.396, Validation Loss: 0.398, Time: 20.0 s\n",
      "Epoch 006: Training Loss: 0.395, Validation Loss: 0.398, Time: 20.0 s\n",
      "Epoch 007: Training Loss: 0.395, Validation Loss: 0.397, Time: 20.6 s\n",
      "Epoch 008: Training Loss: 0.394, Validation Loss: 0.397, Time: 20.2 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 009: Training Loss: 0.393, Validation Loss: 0.396, Time: 20.2 s\n",
      "Epoch 010: Training Loss: 0.393, Validation Loss: 0.396, Time: 20.9 s\n",
      "Epoch 011: Training Loss: 0.393, Validation Loss: 0.397, Time: 21.3 s\n",
      "Epoch 012: Training Loss: 0.393, Validation Loss: 0.396, Time: 20.6 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 013: Training Loss: 0.393, Validation Loss: 0.396, Time: 19.6 s\n",
      "Epoch 014: Training Loss: 0.393, Validation Loss: 0.396, Time: 19.8 s\n",
      "Epoch 015: Training Loss: 0.393, Validation Loss: 0.396, Time: 20.3 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 016: Training Loss: 0.392, Validation Loss: 0.396, Time: 20.1 s\n",
      "Epoch 017: Training Loss: 0.393, Validation Loss: 0.396, Time: 19.9 s\n",
      "Epoch 018: Training Loss: 0.392, Validation Loss: 0.396, Time: 19.7 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 387.14 seconds\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Training LVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.390, Validation Loss: 0.380, Time: 151.1 s\n",
      "Epoch 001: Training Loss: 0.381, Validation Loss: 0.379, Time: 134.6 s\n",
      "Epoch 002: Training Loss: 0.380, Validation Loss: 0.379, Time: 131.4 s\n",
      "Epoch 003: Training Loss: 0.380, Validation Loss: 0.378, Time: 149.6 s\n",
      "Epoch 004: Training Loss: 0.379, Validation Loss: 0.379, Time: 132.1 s\n",
      "Epoch 005: Training Loss: 0.379, Validation Loss: 0.379, Time: 144.7 s\n",
      "Epoch 006: Training Loss: 0.379, Validation Loss: 0.377, Time: 137.6 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 007: Training Loss: 0.378, Validation Loss: 0.377, Time: 143.4 s\n",
      "Epoch 008: Training Loss: 0.378, Validation Loss: 0.377, Time: 153.2 s\n",
      "Epoch 009: Training Loss: 0.378, Validation Loss: 0.377, Time: 139.8 s\n",
      "Epoch 010: Training Loss: 0.378, Validation Loss: 0.377, Time: 129.3 s\n",
      "Epoch 011: Training Loss: 0.378, Validation Loss: 0.378, Time: 130.0 s\n",
      "Epoch 012: Training Loss: 0.378, Validation Loss: 0.378, Time: 131.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 013: Training Loss: 0.377, Validation Loss: 0.377, Time: 142.2 s\n",
      "Epoch 014: Training Loss: 0.377, Validation Loss: 0.377, Time: 139.7 s\n",
      "Epoch 015: Training Loss: 0.377, Validation Loss: 0.376, Time: 134.3 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 016: Training Loss: 0.377, Validation Loss: 0.377, Time: 143.2 s\n",
      "Epoch 017: Training Loss: 0.377, Validation Loss: 0.377, Time: 156.3 s\n",
      "Epoch 018: Training Loss: 0.377, Validation Loss: 0.377, Time: 148.3 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 2672.6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.808, Validation Loss: 0.745, Time: 14.7 s\n",
      "Epoch 001: Training Loss: 0.718, Validation Loss: 0.713, Time: 12.4 s\n",
      "Epoch 002: Training Loss: 0.700, Validation Loss: 0.702, Time: 12.4 s\n",
      "Epoch 003: Training Loss: 0.691, Validation Loss: 0.696, Time: 12.4 s\n",
      "Epoch 004: Training Loss: 0.685, Validation Loss: 0.691, Time: 12.4 s\n",
      "Epoch 005: Training Loss: 0.681, Validation Loss: 0.686, Time: 12.4 s\n",
      "Epoch 006: Training Loss: 0.677, Validation Loss: 0.683, Time: 12.4 s\n",
      "Epoch 007: Training Loss: 0.673, Validation Loss: 0.680, Time: 12.6 s\n",
      "Epoch 008: Training Loss: 0.670, Validation Loss: 0.678, Time: 12.5 s\n",
      "Epoch 009: Training Loss: 0.668, Validation Loss: 0.675, Time: 12.3 s\n",
      "Epoch 010: Training Loss: 0.665, Validation Loss: 0.673, Time: 12.5 s\n",
      "Epoch 011: Training Loss: 0.663, Validation Loss: 0.671, Time: 12.4 s\n",
      "Epoch 012: Training Loss: 0.661, Validation Loss: 0.669, Time: 12.5 s\n",
      "Epoch 013: Training Loss: 0.658, Validation Loss: 0.667, Time: 12.4 s\n",
      "Epoch 014: Training Loss: 0.657, Validation Loss: 0.666, Time: 12.4 s\n",
      "Epoch 015: Training Loss: 0.655, Validation Loss: 0.664, Time: 12.4 s\n",
      "Epoch 016: Training Loss: 0.653, Validation Loss: 0.663, Time: 12.4 s\n",
      "Epoch 017: Training Loss: 0.652, Validation Loss: 0.661, Time: 12.6 s\n",
      "Epoch 018: Training Loss: 0.650, Validation Loss: 0.660, Time: 12.3 s\n",
      "Epoch 019: Training Loss: 0.649, Validation Loss: 0.658, Time: 12.3 s\n",
      "Epoch 020: Training Loss: 0.648, Validation Loss: 0.658, Time: 12.5 s\n",
      "Epoch 021: Training Loss: 0.646, Validation Loss: 0.656, Time: 12.4 s\n",
      "Epoch 022: Training Loss: 0.645, Validation Loss: 0.655, Time: 12.4 s\n",
      "Epoch 023: Training Loss: 0.644, Validation Loss: 0.654, Time: 12.3 s\n",
      "Epoch 024: Training Loss: 0.643, Validation Loss: 0.653, Time: 12.3 s\n",
      "Epoch 025: Training Loss: 0.642, Validation Loss: 0.652, Time: 12.3 s\n",
      "Epoch 026: Training Loss: 0.641, Validation Loss: 0.652, Time: 12.4 s\n",
      "Epoch 027: Training Loss: 0.640, Validation Loss: 0.650, Time: 12.6 s\n",
      "Epoch 028: Training Loss: 0.639, Validation Loss: 0.649, Time: 12.4 s\n",
      "Epoch 029: Training Loss: 0.638, Validation Loss: 0.648, Time: 12.3 s\n",
      "Epoch 030: Training Loss: 0.637, Validation Loss: 0.647, Time: 12.4 s\n",
      "Epoch 031: Training Loss: 0.636, Validation Loss: 0.647, Time: 12.4 s\n",
      "Epoch 032: Training Loss: 0.635, Validation Loss: 0.646, Time: 12.3 s\n",
      "Epoch 033: Training Loss: 0.634, Validation Loss: 0.645, Time: 12.3 s\n",
      "Epoch 034: Training Loss: 0.634, Validation Loss: 0.644, Time: 12.4 s\n",
      "Epoch 035: Training Loss: 0.633, Validation Loss: 0.643, Time: 12.3 s\n",
      "Epoch 036: Training Loss: 0.632, Validation Loss: 0.643, Time: 12.4 s\n",
      "Epoch 037: Training Loss: 0.631, Validation Loss: 0.642, Time: 12.5 s\n",
      "Epoch 038: Training Loss: 0.631, Validation Loss: 0.641, Time: 12.3 s\n",
      "Epoch 039: Training Loss: 0.630, Validation Loss: 0.641, Time: 12.4 s\n",
      "Epoch 040: Training Loss: 0.629, Validation Loss: 0.640, Time: 12.4 s\n",
      "Epoch 041: Training Loss: 0.629, Validation Loss: 0.640, Time: 12.3 s\n",
      "Epoch 042: Training Loss: 0.628, Validation Loss: 0.639, Time: 12.3 s\n",
      "Epoch 043: Training Loss: 0.628, Validation Loss: 0.639, Time: 12.3 s\n",
      "Epoch 044: Training Loss: 0.627, Validation Loss: 0.638, Time: 12.4 s\n",
      "Epoch 045: Training Loss: 0.627, Validation Loss: 0.638, Time: 12.3 s\n",
      "Epoch 046: Training Loss: 0.626, Validation Loss: 0.637, Time: 12.5 s\n",
      "Epoch 047: Training Loss: 0.626, Validation Loss: 0.637, Time: 12.3 s\n",
      "Epoch 048: Training Loss: 0.625, Validation Loss: 0.637, Time: 12.3 s\n",
      "Epoch 049: Training Loss: 0.625, Validation Loss: 0.636, Time: 12.4 s\n",
      "Epoch 050: Training Loss: 0.624, Validation Loss: 0.635, Time: 12.3 s\n",
      "Epoch 051: Training Loss: 0.624, Validation Loss: 0.635, Time: 12.4 s\n",
      "Epoch 052: Training Loss: 0.623, Validation Loss: 0.635, Time: 12.3 s\n",
      "Epoch 053: Training Loss: 0.623, Validation Loss: 0.634, Time: 12.3 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 054: Training Loss: 0.622, Validation Loss: 0.634, Time: 12.3 s\n",
      "Epoch 055: Training Loss: 0.621, Validation Loss: 0.634, Time: 12.3 s\n",
      "Epoch 056: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.6 s\n",
      "Epoch 057: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.3 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 058: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.3 s\n",
      "Epoch 059: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.4 s\n",
      "Epoch 060: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.4 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 061: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.3 s\n",
      "Epoch 062: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.3 s\n",
      "Epoch 063: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.4 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 794.52 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,305\n",
      "Trainable params: 5,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.819, Validation Cluster: 0.798, Validation AE: 0.772], Label Change: 0.030, Time: 474.3 s\n",
      "Iter 001 Loss: [Training: 0.903, Validation Cluster: 0.873, Validation AE: 0.782], Label Change: 0.040, Time: 353.6 s\n",
      "Iter 002 Loss: [Training: 0.944, Validation Cluster: 0.918, Validation AE: 0.788], Label Change: 0.018, Time: 334.6 s\n",
      "Iter 003 Loss: [Training: 0.955, Validation Cluster: 0.936, Validation AE: 0.791], Label Change: 0.009, Time: 341.6 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.959, Validation Cluster: 0.952, Validation AE: 0.792], Label Change: 0.006, Time: 353.9 s\n",
      "Iter 005 Loss: [Training: 0.958, Validation Cluster: 0.952, Validation AE: 0.792], Label Change: 0.004, Time: 355.2 s\n",
      "Iter 006 Loss: [Training: 0.956, Validation Cluster: 0.951, Validation AE: 0.793], Label Change: 0.003, Time: 345.7 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.7926451 not improving.\n",
      "Proportion of Labels Changed:  0.0025550713390245358  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     24835\n",
      "1     26478\n",
      "2      9380\n",
      "3      5364\n",
      "4      3125\n",
      "5      2635\n",
      "6      2305\n",
      "7      1687\n",
      "8      1372\n",
      "9      1633\n",
      "10     4941\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 3487.7282240390778\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.808, Validation Loss: 0.745, Time: 18.8 s\n",
      "Epoch 001: Training Loss: 0.718, Validation Loss: 0.713, Time: 12.5 s\n",
      "Epoch 002: Training Loss: 0.700, Validation Loss: 0.702, Time: 12.2 s\n",
      "Epoch 003: Training Loss: 0.691, Validation Loss: 0.696, Time: 12.2 s\n",
      "Epoch 004: Training Loss: 0.685, Validation Loss: 0.691, Time: 12.3 s\n",
      "Epoch 005: Training Loss: 0.681, Validation Loss: 0.686, Time: 12.3 s\n",
      "Epoch 006: Training Loss: 0.677, Validation Loss: 0.683, Time: 12.3 s\n",
      "Epoch 007: Training Loss: 0.673, Validation Loss: 0.680, Time: 12.3 s\n",
      "Epoch 008: Training Loss: 0.670, Validation Loss: 0.678, Time: 12.2 s\n",
      "Epoch 009: Training Loss: 0.668, Validation Loss: 0.675, Time: 12.1 s\n",
      "Epoch 010: Training Loss: 0.665, Validation Loss: 0.673, Time: 12.1 s\n",
      "Epoch 011: Training Loss: 0.663, Validation Loss: 0.671, Time: 12.3 s\n",
      "Epoch 012: Training Loss: 0.661, Validation Loss: 0.669, Time: 12.2 s\n",
      "Epoch 013: Training Loss: 0.658, Validation Loss: 0.667, Time: 12.1 s\n",
      "Epoch 014: Training Loss: 0.657, Validation Loss: 0.666, Time: 12.0 s\n",
      "Epoch 015: Training Loss: 0.655, Validation Loss: 0.664, Time: 12.2 s\n",
      "Epoch 016: Training Loss: 0.653, Validation Loss: 0.663, Time: 12.4 s\n",
      "Epoch 017: Training Loss: 0.652, Validation Loss: 0.661, Time: 12.1 s\n",
      "Epoch 018: Training Loss: 0.650, Validation Loss: 0.660, Time: 12.1 s\n",
      "Epoch 019: Training Loss: 0.649, Validation Loss: 0.658, Time: 12.2 s\n",
      "Epoch 020: Training Loss: 0.648, Validation Loss: 0.658, Time: 12.2 s\n",
      "Epoch 021: Training Loss: 0.646, Validation Loss: 0.656, Time: 12.1 s\n",
      "Epoch 022: Training Loss: 0.645, Validation Loss: 0.655, Time: 12.1 s\n",
      "Epoch 023: Training Loss: 0.644, Validation Loss: 0.654, Time: 12.1 s\n",
      "Epoch 024: Training Loss: 0.643, Validation Loss: 0.653, Time: 12.1 s\n",
      "Epoch 025: Training Loss: 0.642, Validation Loss: 0.652, Time: 12.4 s\n",
      "Epoch 026: Training Loss: 0.641, Validation Loss: 0.652, Time: 12.4 s\n",
      "Epoch 027: Training Loss: 0.640, Validation Loss: 0.650, Time: 12.1 s\n",
      "Epoch 028: Training Loss: 0.639, Validation Loss: 0.649, Time: 12.1 s\n",
      "Epoch 029: Training Loss: 0.638, Validation Loss: 0.648, Time: 12.2 s\n",
      "Epoch 030: Training Loss: 0.637, Validation Loss: 0.647, Time: 12.1 s\n",
      "Epoch 031: Training Loss: 0.636, Validation Loss: 0.647, Time: 12.2 s\n",
      "Epoch 032: Training Loss: 0.635, Validation Loss: 0.646, Time: 12.1 s\n",
      "Epoch 033: Training Loss: 0.634, Validation Loss: 0.645, Time: 12.1 s\n",
      "Epoch 034: Training Loss: 0.634, Validation Loss: 0.644, Time: 12.1 s\n",
      "Epoch 035: Training Loss: 0.633, Validation Loss: 0.643, Time: 12.4 s\n",
      "Epoch 036: Training Loss: 0.632, Validation Loss: 0.643, Time: 12.3 s\n",
      "Epoch 037: Training Loss: 0.631, Validation Loss: 0.642, Time: 12.3 s\n",
      "Epoch 038: Training Loss: 0.631, Validation Loss: 0.641, Time: 12.3 s\n",
      "Epoch 039: Training Loss: 0.630, Validation Loss: 0.641, Time: 12.1 s\n",
      "Epoch 040: Training Loss: 0.629, Validation Loss: 0.640, Time: 12.1 s\n",
      "Epoch 041: Training Loss: 0.629, Validation Loss: 0.640, Time: 12.2 s\n",
      "Epoch 042: Training Loss: 0.628, Validation Loss: 0.639, Time: 12.1 s\n",
      "Epoch 043: Training Loss: 0.628, Validation Loss: 0.639, Time: 12.1 s\n",
      "Epoch 044: Training Loss: 0.627, Validation Loss: 0.638, Time: 12.1 s\n",
      "Epoch 045: Training Loss: 0.627, Validation Loss: 0.638, Time: 12.6 s\n",
      "Epoch 046: Training Loss: 0.626, Validation Loss: 0.637, Time: 12.2 s\n",
      "Epoch 047: Training Loss: 0.626, Validation Loss: 0.637, Time: 12.1 s\n",
      "Epoch 048: Training Loss: 0.625, Validation Loss: 0.637, Time: 12.2 s\n",
      "Epoch 049: Training Loss: 0.625, Validation Loss: 0.636, Time: 12.2 s\n",
      "Epoch 050: Training Loss: 0.624, Validation Loss: 0.635, Time: 12.2 s\n",
      "Epoch 051: Training Loss: 0.624, Validation Loss: 0.635, Time: 12.1 s\n",
      "Epoch 052: Training Loss: 0.623, Validation Loss: 0.635, Time: 12.2 s\n",
      "Epoch 053: Training Loss: 0.623, Validation Loss: 0.634, Time: 12.1 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 054: Training Loss: 0.622, Validation Loss: 0.634, Time: 12.1 s\n",
      "Epoch 055: Training Loss: 0.621, Validation Loss: 0.634, Time: 12.6 s\n",
      "Epoch 056: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.2 s\n",
      "Epoch 057: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.2 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 058: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.2 s\n",
      "Epoch 059: Training Loss: 0.621, Validation Loss: 0.633, Time: 12.1 s\n",
      "Epoch 060: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.2 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 061: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.3 s\n",
      "Epoch 062: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.1 s\n",
      "Epoch 063: Training Loss: 0.620, Validation Loss: 0.633, Time: 12.1 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 787.8 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,305\n",
      "Trainable params: 5,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.819, Validation Cluster: 0.798, Validation AE: 0.772], Label Change: 0.030, Time: 378.2 s\n",
      "Iter 001 Loss: [Training: 0.903, Validation Cluster: 0.873, Validation AE: 0.782], Label Change: 0.040, Time: 353.6 s\n",
      "Iter 002 Loss: [Training: 0.944, Validation Cluster: 0.918, Validation AE: 0.788], Label Change: 0.018, Time: 346.4 s\n",
      "Iter 003 Loss: [Training: 0.955, Validation Cluster: 0.936, Validation AE: 0.791], Label Change: 0.009, Time: 344.4 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.959, Validation Cluster: 0.952, Validation AE: 0.792], Label Change: 0.006, Time: 355.7 s\n",
      "Iter 005 Loss: [Training: 0.958, Validation Cluster: 0.952, Validation AE: 0.792], Label Change: 0.004, Time: 347.8 s\n",
      "Iter 006 Loss: [Training: 0.956, Validation Cluster: 0.951, Validation AE: 0.793], Label Change: 0.003, Time: 359.0 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.7926451 not improving.\n",
      "Proportion of Labels Changed:  0.0025550713390245358  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     24835\n",
      "1     26478\n",
      "2      9380\n",
      "3      5364\n",
      "4      3125\n",
      "5      2635\n",
      "6      2305\n",
      "7      1687\n",
      "8      1372\n",
      "9      1633\n",
      "10     4941\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 3395.938683271408\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n",
      "Weight files for count models not detected. Training HVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.401, Validation Loss: 0.390, Time: 31.3 s\n",
      "Epoch 001: Training Loss: 0.387, Validation Loss: 0.385, Time: 26.7 s\n",
      "Epoch 002: Training Loss: 0.383, Validation Loss: 0.383, Time: 26.6 s\n",
      "Epoch 003: Training Loss: 0.381, Validation Loss: 0.381, Time: 26.5 s\n",
      "Epoch 004: Training Loss: 0.379, Validation Loss: 0.380, Time: 26.8 s\n",
      "Epoch 005: Training Loss: 0.378, Validation Loss: 0.379, Time: 26.9 s\n",
      "Epoch 006: Training Loss: 0.377, Validation Loss: 0.378, Time: 26.4 s\n",
      "Epoch 007: Training Loss: 0.376, Validation Loss: 0.377, Time: 26.3 s\n",
      "Epoch 008: Training Loss: 0.376, Validation Loss: 0.376, Time: 26.3 s\n",
      "Epoch 009: Training Loss: 0.375, Validation Loss: 0.376, Time: 27.3 s\n",
      "Epoch 010: Training Loss: 0.374, Validation Loss: 0.375, Time: 26.2 s\n",
      "Epoch 011: Training Loss: 0.374, Validation Loss: 0.375, Time: 26.6 s\n",
      "Epoch 012: Training Loss: 0.374, Validation Loss: 0.375, Time: 26.3 s\n",
      "Epoch 013: Training Loss: 0.373, Validation Loss: 0.375, Time: 26.6 s\n",
      "Epoch 014: Training Loss: 0.373, Validation Loss: 0.374, Time: 26.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 015: Training Loss: 0.372, Validation Loss: 0.374, Time: 26.6 s\n",
      "Epoch 016: Training Loss: 0.372, Validation Loss: 0.374, Time: 26.3 s\n",
      "Epoch 017: Training Loss: 0.372, Validation Loss: 0.374, Time: 26.1 s\n",
      "Epoch 018: Training Loss: 0.372, Validation Loss: 0.373, Time: 27.4 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 019: Training Loss: 0.372, Validation Loss: 0.373, Time: 26.3 s\n",
      "Epoch 020: Training Loss: 0.371, Validation Loss: 0.373, Time: 26.4 s\n",
      "Epoch 021: Training Loss: 0.371, Validation Loss: 0.373, Time: 26.6 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 022: Training Loss: 0.371, Validation Loss: 0.373, Time: 26.9 s\n",
      "Epoch 023: Training Loss: 0.371, Validation Loss: 0.373, Time: 27.0 s\n",
      "Epoch 024: Training Loss: 0.371, Validation Loss: 0.373, Time: 26.7 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 670.19 seconds\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Training LVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.388, Validation Loss: 0.380, Time: 198.4 s\n",
      "Epoch 001: Training Loss: 0.381, Validation Loss: 0.379, Time: 197.5 s\n",
      "Epoch 002: Training Loss: 0.380, Validation Loss: 0.379, Time: 201.3 s\n",
      "Epoch 003: Training Loss: 0.380, Validation Loss: 0.379, Time: 200.5 s\n",
      "Epoch 004: Training Loss: 0.379, Validation Loss: 0.378, Time: 196.7 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 005: Training Loss: 0.378, Validation Loss: 0.378, Time: 220.0 s\n",
      "Epoch 006: Training Loss: 0.378, Validation Loss: 0.378, Time: 212.5 s\n",
      "Epoch 007: Training Loss: 0.378, Validation Loss: 0.378, Time: 196.4 s\n",
      "Epoch 008: Training Loss: 0.378, Validation Loss: 0.378, Time: 218.6 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 009: Training Loss: 0.378, Validation Loss: 0.377, Time: 227.8 s\n",
      "Epoch 010: Training Loss: 0.378, Validation Loss: 0.377, Time: 208.2 s\n",
      "Epoch 011: Training Loss: 0.378, Validation Loss: 0.377, Time: 191.2 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 012: Training Loss: 0.378, Validation Loss: 0.377, Time: 210.5 s\n",
      "Epoch 013: Training Loss: 0.378, Validation Loss: 0.377, Time: 224.3 s\n",
      "Epoch 014: Training Loss: 0.378, Validation Loss: 0.377, Time: 187.3 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 3091.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.797, Validation Loss: 0.715, Time: 18.9 s\n",
      "Epoch 001: Training Loss: 0.711, Validation Loss: 0.691, Time: 15.8 s\n",
      "Epoch 002: Training Loss: 0.696, Validation Loss: 0.682, Time: 15.3 s\n",
      "Epoch 003: Training Loss: 0.688, Validation Loss: 0.676, Time: 15.3 s\n",
      "Epoch 004: Training Loss: 0.682, Validation Loss: 0.672, Time: 15.2 s\n",
      "Epoch 005: Training Loss: 0.677, Validation Loss: 0.667, Time: 15.7 s\n",
      "Epoch 006: Training Loss: 0.673, Validation Loss: 0.665, Time: 15.4 s\n",
      "Epoch 007: Training Loss: 0.670, Validation Loss: 0.661, Time: 15.4 s\n",
      "Epoch 008: Training Loss: 0.666, Validation Loss: 0.659, Time: 15.4 s\n",
      "Epoch 009: Training Loss: 0.664, Validation Loss: 0.657, Time: 15.2 s\n",
      "Epoch 010: Training Loss: 0.661, Validation Loss: 0.653, Time: 15.3 s\n",
      "Epoch 011: Training Loss: 0.659, Validation Loss: 0.651, Time: 15.3 s\n",
      "Epoch 012: Training Loss: 0.657, Validation Loss: 0.650, Time: 15.2 s\n",
      "Epoch 013: Training Loss: 0.655, Validation Loss: 0.648, Time: 15.8 s\n",
      "Epoch 014: Training Loss: 0.653, Validation Loss: 0.646, Time: 15.3 s\n",
      "Epoch 015: Training Loss: 0.651, Validation Loss: 0.644, Time: 15.5 s\n",
      "Epoch 016: Training Loss: 0.650, Validation Loss: 0.643, Time: 15.4 s\n",
      "Epoch 017: Training Loss: 0.648, Validation Loss: 0.641, Time: 15.3 s\n",
      "Epoch 018: Training Loss: 0.646, Validation Loss: 0.640, Time: 15.2 s\n",
      "Epoch 019: Training Loss: 0.645, Validation Loss: 0.638, Time: 15.3 s\n",
      "Epoch 020: Training Loss: 0.643, Validation Loss: 0.636, Time: 15.5 s\n",
      "Epoch 021: Training Loss: 0.642, Validation Loss: 0.635, Time: 15.5 s\n",
      "Epoch 022: Training Loss: 0.641, Validation Loss: 0.634, Time: 15.2 s\n",
      "Epoch 023: Training Loss: 0.640, Validation Loss: 0.633, Time: 15.5 s\n",
      "Epoch 024: Training Loss: 0.639, Validation Loss: 0.631, Time: 15.5 s\n",
      "Epoch 025: Training Loss: 0.637, Validation Loss: 0.631, Time: 15.3 s\n",
      "Epoch 026: Training Loss: 0.637, Validation Loss: 0.630, Time: 15.4 s\n",
      "Epoch 027: Training Loss: 0.636, Validation Loss: 0.629, Time: 15.3 s\n",
      "Epoch 028: Training Loss: 0.635, Validation Loss: 0.628, Time: 15.6 s\n",
      "Epoch 029: Training Loss: 0.634, Validation Loss: 0.627, Time: 15.4 s\n",
      "Epoch 030: Training Loss: 0.633, Validation Loss: 0.626, Time: 15.3 s\n",
      "Epoch 031: Training Loss: 0.632, Validation Loss: 0.626, Time: 15.5 s\n",
      "Epoch 032: Training Loss: 0.631, Validation Loss: 0.625, Time: 15.4 s\n",
      "Epoch 033: Training Loss: 0.630, Validation Loss: 0.624, Time: 15.4 s\n",
      "Epoch 034: Training Loss: 0.630, Validation Loss: 0.625, Time: 15.4 s\n",
      "Epoch 035: Training Loss: 0.629, Validation Loss: 0.623, Time: 15.3 s\n",
      "Epoch 036: Training Loss: 0.628, Validation Loss: 0.622, Time: 15.8 s\n",
      "Epoch 037: Training Loss: 0.628, Validation Loss: 0.621, Time: 15.4 s\n",
      "Epoch 038: Training Loss: 0.627, Validation Loss: 0.621, Time: 15.3 s\n",
      "Epoch 039: Training Loss: 0.627, Validation Loss: 0.621, Time: 15.5 s\n",
      "Epoch 040: Training Loss: 0.626, Validation Loss: 0.620, Time: 15.5 s\n",
      "Epoch 041: Training Loss: 0.626, Validation Loss: 0.620, Time: 15.3 s\n",
      "Epoch 042: Training Loss: 0.625, Validation Loss: 0.619, Time: 15.4 s\n",
      "Epoch 043: Training Loss: 0.625, Validation Loss: 0.619, Time: 15.3 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 044: Training Loss: 0.623, Validation Loss: 0.618, Time: 15.9 s\n",
      "Epoch 045: Training Loss: 0.623, Validation Loss: 0.618, Time: 15.4 s\n",
      "Epoch 046: Training Loss: 0.623, Validation Loss: 0.618, Time: 15.5 s\n",
      "Epoch 047: Training Loss: 0.623, Validation Loss: 0.617, Time: 15.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 048: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.4 s\n",
      "Epoch 049: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.4 s\n",
      "Epoch 050: Training Loss: 0.622, Validation Loss: 0.618, Time: 15.4 s\n",
      "Epoch 051: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.3 s\n",
      "Epoch 052: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.6 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 053: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.3 s\n",
      "Epoch 054: Training Loss: 0.622, Validation Loss: 0.618, Time: 15.5 s\n",
      "Epoch 055: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 056: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.4 s\n",
      "Epoch 057: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.3 s\n",
      "Epoch 058: Training Loss: 0.622, Validation Loss: 0.618, Time: 15.3 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 912.54 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,305\n",
      "Trainable params: 5,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.816, Validation Cluster: 0.801, Validation AE: 0.778], Label Change: 0.033, Time: 521.2 s\n",
      "Iter 001 Loss: [Training: 0.894, Validation Cluster: 0.871, Validation AE: 0.790], Label Change: 0.030, Time: 501.2 s\n",
      "Iter 002 Loss: [Training: 0.943, Validation Cluster: 0.925, Validation AE: 0.797], Label Change: 0.022, Time: 540.4 s\n",
      "Iter 003 Loss: [Training: 0.956, Validation Cluster: 0.945, Validation AE: 0.800], Label Change: 0.013, Time: 529.2 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.961, Validation Cluster: 0.962, Validation AE: 0.801], Label Change: 0.006, Time: 520.1 s\n",
      "Iter 005 Loss: [Training: 0.959, Validation Cluster: 0.962, Validation AE: 0.802], Label Change: 0.004, Time: 525.2 s\n",
      "Iter 006 Loss: [Training: 0.957, Validation Cluster: 0.960, Validation AE: 0.802], Label Change: 0.003, Time: 526.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.80203825 not improving.\n",
      "Proportion of Labels Changed:  0.0030469749937914303  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     30186\n",
      "1     31056\n",
      "2     13229\n",
      "3      6658\n",
      "4      5954\n",
      "5      3605\n",
      "6      2893\n",
      "7      2119\n",
      "8      1757\n",
      "9      2495\n",
      "10     4742\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 5198.396808862686\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n",
      "... storing 'orig.ident' as categorical\n",
      "... storing 'sampleid' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'sorting' as categorical\n",
      "... storing 'lineid' as categorical\n",
      "... storing 'cell.labels' as categorical\n",
      "... storing 'barcode' as categorical\n",
      "... storing 'Time' as categorical\n",
      "... storing 'Disease' as categorical\n",
      "... storing 'Fact.sorting' as categorical\n",
      "... storing 'Sex' as categorical\n",
      "... storing 'Time2' as categorical\n",
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:913: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain weight index file not detected, pretraining autoencoder weights.\n",
      "\n",
      "Epoch 000: Training Loss: 0.797, Validation Loss: 0.715, Time: 31.5 s\n",
      "Epoch 001: Training Loss: 0.711, Validation Loss: 0.691, Time: 15.4 s\n",
      "Epoch 002: Training Loss: 0.696, Validation Loss: 0.682, Time: 15.2 s\n",
      "Epoch 003: Training Loss: 0.688, Validation Loss: 0.676, Time: 15.2 s\n",
      "Epoch 004: Training Loss: 0.682, Validation Loss: 0.672, Time: 15.2 s\n",
      "Epoch 005: Training Loss: 0.677, Validation Loss: 0.667, Time: 15.3 s\n",
      "Epoch 006: Training Loss: 0.673, Validation Loss: 0.665, Time: 15.6 s\n",
      "Epoch 007: Training Loss: 0.670, Validation Loss: 0.661, Time: 15.4 s\n",
      "Epoch 008: Training Loss: 0.666, Validation Loss: 0.659, Time: 15.3 s\n",
      "Epoch 009: Training Loss: 0.664, Validation Loss: 0.657, Time: 15.4 s\n",
      "Epoch 010: Training Loss: 0.661, Validation Loss: 0.653, Time: 15.2 s\n",
      "Epoch 011: Training Loss: 0.659, Validation Loss: 0.651, Time: 15.2 s\n",
      "Epoch 012: Training Loss: 0.657, Validation Loss: 0.650, Time: 15.2 s\n",
      "Epoch 013: Training Loss: 0.655, Validation Loss: 0.648, Time: 15.2 s\n",
      "Epoch 014: Training Loss: 0.653, Validation Loss: 0.646, Time: 15.6 s\n",
      "Epoch 015: Training Loss: 0.651, Validation Loss: 0.644, Time: 15.3 s\n",
      "Epoch 016: Training Loss: 0.650, Validation Loss: 0.643, Time: 15.4 s\n",
      "Epoch 017: Training Loss: 0.648, Validation Loss: 0.641, Time: 15.3 s\n",
      "Epoch 018: Training Loss: 0.646, Validation Loss: 0.640, Time: 15.3 s\n",
      "Epoch 019: Training Loss: 0.645, Validation Loss: 0.638, Time: 15.2 s\n",
      "Epoch 020: Training Loss: 0.643, Validation Loss: 0.636, Time: 15.4 s\n",
      "Epoch 021: Training Loss: 0.642, Validation Loss: 0.635, Time: 15.2 s\n",
      "Epoch 022: Training Loss: 0.641, Validation Loss: 0.634, Time: 15.8 s\n",
      "Epoch 023: Training Loss: 0.640, Validation Loss: 0.633, Time: 15.1 s\n",
      "Epoch 024: Training Loss: 0.639, Validation Loss: 0.631, Time: 15.3 s\n",
      "Epoch 025: Training Loss: 0.637, Validation Loss: 0.631, Time: 15.4 s\n",
      "Epoch 026: Training Loss: 0.637, Validation Loss: 0.630, Time: 15.3 s\n",
      "Epoch 027: Training Loss: 0.636, Validation Loss: 0.629, Time: 15.2 s\n",
      "Epoch 028: Training Loss: 0.635, Validation Loss: 0.628, Time: 15.2 s\n",
      "Epoch 029: Training Loss: 0.634, Validation Loss: 0.627, Time: 15.3 s\n",
      "Epoch 030: Training Loss: 0.633, Validation Loss: 0.626, Time: 15.5 s\n",
      "Epoch 031: Training Loss: 0.632, Validation Loss: 0.626, Time: 15.2 s\n",
      "Epoch 032: Training Loss: 0.631, Validation Loss: 0.625, Time: 15.3 s\n",
      "Epoch 033: Training Loss: 0.630, Validation Loss: 0.624, Time: 15.3 s\n",
      "Epoch 034: Training Loss: 0.630, Validation Loss: 0.625, Time: 15.2 s\n",
      "Epoch 035: Training Loss: 0.629, Validation Loss: 0.623, Time: 15.3 s\n",
      "Epoch 036: Training Loss: 0.628, Validation Loss: 0.622, Time: 15.2 s\n",
      "Epoch 037: Training Loss: 0.628, Validation Loss: 0.621, Time: 15.4 s\n",
      "Epoch 038: Training Loss: 0.627, Validation Loss: 0.621, Time: 15.3 s\n",
      "Epoch 039: Training Loss: 0.627, Validation Loss: 0.621, Time: 15.2 s\n",
      "Epoch 040: Training Loss: 0.626, Validation Loss: 0.620, Time: 15.3 s\n",
      "Epoch 041: Training Loss: 0.626, Validation Loss: 0.620, Time: 15.3 s\n",
      "Epoch 042: Training Loss: 0.625, Validation Loss: 0.619, Time: 15.2 s\n",
      "Epoch 043: Training Loss: 0.625, Validation Loss: 0.619, Time: 15.2 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Epoch 044: Training Loss: 0.623, Validation Loss: 0.618, Time: 15.2 s\n",
      "Epoch 045: Training Loss: 0.623, Validation Loss: 0.618, Time: 15.5 s\n",
      "Epoch 046: Training Loss: 0.623, Validation Loss: 0.618, Time: 15.3 s\n",
      "Epoch 047: Training Loss: 0.623, Validation Loss: 0.617, Time: 15.3 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "Epoch 048: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.3 s\n",
      "Epoch 049: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.3 s\n",
      "Epoch 050: Training Loss: 0.622, Validation Loss: 0.618, Time: 15.2 s\n",
      "Epoch 051: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.2 s\n",
      "Epoch 052: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.3 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-06\n",
      "Epoch 053: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.7 s\n",
      "Epoch 054: Training Loss: 0.622, Validation Loss: 0.618, Time: 15.3 s\n",
      "Epoch 055: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.4 s\n",
      "\n",
      "Decaying Learning Rate to: 1.234568e-06\n",
      "Epoch 056: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.4 s\n",
      "Epoch 057: Training Loss: 0.622, Validation Loss: 0.617, Time: 15.2 s\n",
      "Epoch 058: Training Loss: 0.622, Validation Loss: 0.618, Time: 15.3 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 919.5 seconds\n",
      "\n",
      "Initializing cluster centroids using the louvain method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DESCImpute/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../../anaconda3/envs/DESCImpute/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 11 clusters detected. \n",
      "\n",
      "\n",
      "-----------------------CarDEC Architecture-----------------------\n",
      "\n",
      "Model: \"car_dec__model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         multiple                  260256    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         multiple                  262224    \n",
      "_________________________________________________________________\n",
      "encoderLVG (Sequential)      multiple                  2502944   \n",
      "_________________________________________________________________\n",
      "decoderLVG (Sequential)      multiple                  2526529   \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) multiple                  352       \n",
      "=================================================================\n",
      "Total params: 5,552,305\n",
      "Trainable params: 5,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------Encoder Sub-Architecture--------------------\n",
      "\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_0 (Dense)            multiple                  256128    \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 260,256\n",
      "Trainable params: 260,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------Base Decoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder0 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  258000    \n",
      "=================================================================\n",
      "Total params: 262,224\n",
      "Trainable params: 262,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "------------------LVG Encoder Sub-Architecture------------------\n",
      "\n",
      "Model: \"encoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder0 (Dense)             multiple                  2498816   \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            multiple                  4128      \n",
      "=================================================================\n",
      "Total params: 2,502,944\n",
      "Trainable params: 2,502,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "----------------LVG Base Decoder Sub-Architecture----------------\n",
      "\n",
      "Model: \"decoderLVG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoderLVG0 (Dense)          multiple                  8320      \n",
      "_________________________________________________________________\n",
      "outputLVG (Dense)            multiple                  2518209   \n",
      "=================================================================\n",
      "Total params: 2,526,529\n",
      "Trainable params: 2,526,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CarDEC Model Weights not detected. Training full model.\n",
      "\n",
      "Iter 000 Loss: [Training: 0.816, Validation Cluster: 0.801, Validation AE: 0.778], Label Change: 0.033, Time: 540.4 s\n",
      "Iter 001 Loss: [Training: 0.894, Validation Cluster: 0.871, Validation AE: 0.790], Label Change: 0.030, Time: 519.4 s\n",
      "Iter 002 Loss: [Training: 0.943, Validation Cluster: 0.925, Validation AE: 0.797], Label Change: 0.022, Time: 539.3 s\n",
      "Iter 003 Loss: [Training: 0.956, Validation Cluster: 0.945, Validation AE: 0.800], Label Change: 0.013, Time: 534.0 s\n",
      "\n",
      "Decaying Learning Rate to: 3.3333334e-05\n",
      "Iter 004 Loss: [Training: 0.961, Validation Cluster: 0.962, Validation AE: 0.801], Label Change: 0.006, Time: 524.7 s\n",
      "Iter 005 Loss: [Training: 0.959, Validation Cluster: 0.962, Validation AE: 0.802], Label Change: 0.004, Time: 525.8 s\n",
      "Iter 006 Loss: [Training: 0.957, Validation Cluster: 0.960, Validation AE: 0.802], Label Change: 0.003, Time: 529.5 s\n",
      "\n",
      "Decaying Learning Rate to: 1.1111111e-05\n",
      "\n",
      "Autoencoder_loss  0.80203825 not improving.\n",
      "Proportion of Labels Changed:  0.0030469749937914303  is less than tolerance of  0.005\n",
      "\n",
      "Reached tolerance threshold. Stop training.\n",
      "\n",
      "The final cluster assignments are:\n",
      "0     30186\n",
      "1     31056\n",
      "2     13229\n",
      "3      6658\n",
      "4      5954\n",
      "5      3605\n",
      "6      2893\n",
      "7      2119\n",
      "8      1757\n",
      "9      2495\n",
      "10     4742\n",
      "dtype: int64\n",
      "\n",
      "Total Runtime is 5270.535886049271\n",
      "\n",
      "The CarDEC model is now making inference on the data matrix.\n",
      "Inference completed, results added.\n",
      " \n",
      "Weight files for count models not detected. Training HVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.400, Validation Loss: 0.389, Time: 47.9 s\n",
      "Epoch 001: Training Loss: 0.387, Validation Loss: 0.385, Time: 34.8 s\n",
      "Epoch 002: Training Loss: 0.384, Validation Loss: 0.382, Time: 33.4 s\n",
      "Epoch 003: Training Loss: 0.381, Validation Loss: 0.380, Time: 33.8 s\n",
      "Epoch 004: Training Loss: 0.380, Validation Loss: 0.379, Time: 33.7 s\n",
      "Epoch 005: Training Loss: 0.379, Validation Loss: 0.378, Time: 34.2 s\n",
      "Epoch 006: Training Loss: 0.378, Validation Loss: 0.377, Time: 33.6 s\n",
      "Epoch 007: Training Loss: 0.377, Validation Loss: 0.377, Time: 33.8 s\n",
      "Epoch 008: Training Loss: 0.377, Validation Loss: 0.376, Time: 33.7 s\n",
      "Epoch 009: Training Loss: 0.376, Validation Loss: 0.376, Time: 34.3 s\n",
      "Epoch 010: Training Loss: 0.376, Validation Loss: 0.376, Time: 33.9 s\n",
      "Epoch 011: Training Loss: 0.375, Validation Loss: 0.375, Time: 33.9 s\n",
      "Epoch 012: Training Loss: 0.375, Validation Loss: 0.375, Time: 34.7 s\n",
      "Epoch 013: Training Loss: 0.375, Validation Loss: 0.375, Time: 33.2 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 014: Training Loss: 0.374, Validation Loss: 0.374, Time: 33.7 s\n",
      "Epoch 015: Training Loss: 0.374, Validation Loss: 0.374, Time: 34.2 s\n",
      "Epoch 016: Training Loss: 0.374, Validation Loss: 0.374, Time: 33.5 s\n",
      "Epoch 017: Training Loss: 0.373, Validation Loss: 0.374, Time: 33.9 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 018: Training Loss: 0.373, Validation Loss: 0.374, Time: 33.9 s\n",
      "Epoch 019: Training Loss: 0.373, Validation Loss: 0.374, Time: 35.0 s\n",
      "Epoch 020: Training Loss: 0.373, Validation Loss: 0.374, Time: 34.4 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 021: Training Loss: 0.373, Validation Loss: 0.373, Time: 33.9 s\n",
      "Epoch 022: Training Loss: 0.373, Validation Loss: 0.373, Time: 33.7 s\n",
      "Epoch 023: Training Loss: 0.373, Validation Loss: 0.374, Time: 33.9 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 829.12 seconds\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Training LVG count model.\n",
      "\n",
      "Epoch 000: Training Loss: 0.387, Validation Loss: 0.381, Time: 294.0 s\n",
      "Epoch 001: Training Loss: 0.380, Validation Loss: 0.380, Time: 239.9 s\n",
      "Epoch 002: Training Loss: 0.379, Validation Loss: 0.379, Time: 249.3 s\n",
      "Epoch 003: Training Loss: 0.379, Validation Loss: 0.379, Time: 257.0 s\n",
      "Epoch 004: Training Loss: 0.379, Validation Loss: 0.379, Time: 260.3 s\n",
      "Epoch 005: Training Loss: 0.379, Validation Loss: 0.379, Time: 269.1 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00033333336\n",
      "Epoch 006: Training Loss: 0.378, Validation Loss: 0.378, Time: 250.4 s\n",
      "Epoch 007: Training Loss: 0.378, Validation Loss: 0.378, Time: 254.8 s\n",
      "Epoch 008: Training Loss: 0.378, Validation Loss: 0.378, Time: 234.1 s\n",
      "Epoch 009: Training Loss: 0.378, Validation Loss: 0.378, Time: 260.8 s\n",
      "\n",
      "Decaying Learning Rate to: 0.00011111112\n",
      "Epoch 010: Training Loss: 0.377, Validation Loss: 0.378, Time: 227.5 s\n",
      "Epoch 011: Training Loss: 0.377, Validation Loss: 0.378, Time: 258.2 s\n",
      "Epoch 012: Training Loss: 0.377, Validation Loss: 0.378, Time: 255.3 s\n",
      "\n",
      "Decaying Learning Rate to: 3.703704e-05\n",
      "Epoch 013: Training Loss: 0.377, Validation Loss: 0.378, Time: 227.8 s\n",
      "Epoch 014: Training Loss: 0.377, Validation Loss: 0.378, Time: 243.7 s\n",
      "Epoch 015: Training Loss: 0.377, Validation Loss: 0.378, Time: 262.2 s\n",
      "\n",
      "Training Completed\n",
      "Total training time: 4044.15 seconds\n"
     ]
    }
   ],
   "source": [
    "fracs = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "n = 0\n",
    "for frac in fracs:\n",
    "    profile_stats.loc[n], profile_stats.loc[n+1] = profile(frac)\n",
    "    n = n + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (Seconds)</th>\n",
       "      <th>Memory (MiB)</th>\n",
       "      <th>Method</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291.202066</td>\n",
       "      <td>8000.339844</td>\n",
       "      <td>CarDEC Zscore</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1046.707194</td>\n",
       "      <td>7759.339844</td>\n",
       "      <td>CarDEC Count</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502.920225</td>\n",
       "      <td>7993.949219</td>\n",
       "      <td>CarDEC Zscore</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1492.845753</td>\n",
       "      <td>10248.738281</td>\n",
       "      <td>CarDEC Count</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1341.616484</td>\n",
       "      <td>15760.113281</td>\n",
       "      <td>CarDEC Zscore</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3264.819926</td>\n",
       "      <td>18542.656250</td>\n",
       "      <td>CarDEC Count</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2809.305696</td>\n",
       "      <td>20478.750000</td>\n",
       "      <td>CarDEC Zscore</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6043.807879</td>\n",
       "      <td>23409.832031</td>\n",
       "      <td>CarDEC Count</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5034.924791</td>\n",
       "      <td>25158.582031</td>\n",
       "      <td>CarDEC Zscore</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8807.557579</td>\n",
       "      <td>25087.699219</td>\n",
       "      <td>CarDEC Count</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7038.697272</td>\n",
       "      <td>25313.648438</td>\n",
       "      <td>CarDEC Zscore</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12160.816501</td>\n",
       "      <td>25300.253906</td>\n",
       "      <td>CarDEC Count</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time (Seconds)  Memory (MiB)         Method  Percent\n",
       "0       291.202066   8000.339844  CarDEC Zscore     10.0\n",
       "1      1046.707194   7759.339844   CarDEC Count     10.0\n",
       "2       502.920225   7993.949219  CarDEC Zscore     20.0\n",
       "3      1492.845753  10248.738281   CarDEC Count     20.0\n",
       "4      1341.616484  15760.113281  CarDEC Zscore     40.0\n",
       "5      3264.819926  18542.656250   CarDEC Count     40.0\n",
       "6      2809.305696  20478.750000  CarDEC Zscore     60.0\n",
       "7      6043.807879  23409.832031   CarDEC Count     60.0\n",
       "8      5034.924791  25158.582031  CarDEC Zscore     80.0\n",
       "9      8807.557579  25087.699219   CarDEC Count     80.0\n",
       "10     7038.697272  25313.648438  CarDEC Zscore    100.0\n",
       "11    12160.816501  25300.253906   CarDEC Count    100.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_stats.to_csv(\"../Figures/liver/CarDEC_profile.csv\")\n",
    "profile_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cardec] *",
   "language": "python",
   "name": "conda-env-cardec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
