{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCA Liver Runtime\n",
    "\n",
    "In this notebook, we will analyze the scaleability of the DCA method to larger datasets. We fit DCA on various percentages of the Liver dataset (which contains over 100000 cells). We fit DCA on subsets of this data ranging from 10% up to 100% of the full liver data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import RangeIndex\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Broadly useful python packages\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from shutil import move, rmtree\n",
    "import warnings\n",
    "from memory_profiler import memory_usage\n",
    "from time import time\n",
    "\n",
    "\"\"\"Machine learning and single cell packages\"\"\"\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import adjusted_rand_score as ari, normalized_mutual_info_score as nmi\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Miscellaneous useful functions\"\"\"\n",
    "\n",
    "def read_liver_data(path, cache=True):\n",
    "    adata = sc.read_mtx(os.path.join(path, 'matrix.mtx')).T\n",
    "    genes_file = pd.read_csv(os.path.join(path, 'genes.tsv'), sep='\\t')\n",
    "    barcodes_file = pd.read_csv(os.path.join(path, 'barcodes.tsv'), sep='\\t')\n",
    "\n",
    "    adata.var.index = genes_file[\"genename\"]\n",
    "    adata.obs.index = barcodes_file[\"cellname\"]\n",
    "    adata.obs = barcodes_file\n",
    "        \n",
    "    sc.pp.filter_cells(adata, min_genes = 200)\n",
    "    mito_genes = adata.var_names.str.startswith('mt-')\n",
    "    adata.obs['percent_mito'] = np.sum(\n",
    "        adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n",
    "    adata.obs['n_counts'] = adata.X.sum(axis=1).A1\n",
    "    adata = adata[adata.obs['percent_mito'] < 0.2, :]\n",
    "    sc.pp.filter_genes(adata, min_cells = 30)\n",
    "\n",
    "    return adata\n",
    "\n",
    "def build_dir(dir_path):\n",
    "    subdirs = [dir_path]\n",
    "    substring = dir_path\n",
    "\n",
    "    while substring != '':\n",
    "        splt_dir = os.path.split(substring)\n",
    "        substring = splt_dir[0]\n",
    "        subdirs.append(substring)\n",
    "        \n",
    "    subdirs.pop()\n",
    "    subdirs = [x for x in subdirs if os.path.basename(x) != '..']\n",
    "\n",
    "    n = len(subdirs)\n",
    "    subdirs = [subdirs[n - 1 - x] for x in range(n)]\n",
    "    \n",
    "    for dir_ in subdirs:\n",
    "        if not os.path.isdir(dir_):\n",
    "            os.mkdir(dir_)\n",
    "            \n",
    "def run_dca(adata):\n",
    "    sc.external.pp.dca(adata, mode = 'denoise', ae_type='nb-conddisp', verbose = True)\n",
    "        \n",
    "def profile(frac):\n",
    "    np.random.seed(11111)\n",
    "    indices = np.random.choice(range(adata.shape[0]), size = round(frac * adata.shape[0]), replace = False)\n",
    "    tmp = adata.copy()[indices]\n",
    "    \n",
    "    tmp = AnnData(tmp.X.toarray())\n",
    "    sc.pp.filter_genes(tmp, min_cells = 1)\n",
    "    start = time()\n",
    "    run = memory_usage((run_dca, (), {'adata': tmp}))\n",
    "    final = time() - start\n",
    "    peak_memory = max(run) - min(run)\n",
    "    stats_zscore = final, peak_memory, \"DCA\", int(100*frac)\n",
    "    \n",
    "    return stats_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dir(\"../Figures/liver\")\n",
    "profile_stats = {\"Time (Seconds)\": [] , \"Memory (MiB)\": [], \"Method\": [], 'Percent': []}\n",
    "profile_stats = pd.DataFrame(profile_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming to str index.\n",
      "Trying to set attribute `.var` of view, copying.\n"
     ]
    }
   ],
   "source": [
    "adata = read_liver_data(\"../Data/liver\", cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Memory and Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/scanpy/api/__init__.py:6: FutureWarning: \n",
      "\n",
      "In a future version of Scanpy, `scanpy.api` will be removed.\n",
      "Simply use `import scanpy as sc` and `import scanpy.external as sce` instead.\n",
      "\n",
      "  FutureWarning,\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/api.py:149: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/kopt/config.py:60: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  _config = yaml.load(open(_config_path))\n",
      "2020-05-11 13:04:23,803 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/api.py:149: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dca: Successfully preprocessed 21496 genes and 10469 cells.\n",
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:28,821 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/train.py:41: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:29,521 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/train.py:41: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/train.py:41: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:29,523 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/train.py:41: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:87: The name tf.lgamma is deprecated. Please use tf.math.lgamma instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:29,558 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:87: The name tf.lgamma is deprecated. Please use tf.math.lgamma instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:88: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:29,566 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:88: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:10: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:29,575 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:10: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:10: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:29,578 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/dca/loss.py:10: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 21496)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           1375808     count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 21496)        1397240     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 21496)        0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 21496)        1397240     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 21496)        0           lambda_2[0][0]                   \n",
      "                                                                 dispersion[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,174,960\n",
      "Trainable params: 4,174,640\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 13:04:30,667 [WARNING] From /Users/jlakkis/anaconda3/envs/DavidDESC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9422 samples, validate on 1047 samples\n",
      "Epoch 1/300\n",
      "9422/9422 [==============================] - 22s 2ms/step - loss: 0.4646 - val_loss: 0.4084\n",
      "Epoch 2/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3920 - val_loss: 0.3970\n",
      "Epoch 3/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3878 - val_loss: 0.3938\n",
      "Epoch 4/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3856 - val_loss: 0.3911\n",
      "Epoch 5/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3845 - val_loss: 0.3923\n",
      "Epoch 6/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3833 - val_loss: 0.3906\n",
      "Epoch 7/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3827 - val_loss: 0.3899\n",
      "Epoch 8/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3821 - val_loss: 0.3885\n",
      "Epoch 9/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3816 - val_loss: 0.3899\n",
      "Epoch 10/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3812 - val_loss: 0.3884\n",
      "Epoch 11/300\n",
      "9422/9422 [==============================] - 24s 2ms/step - loss: 0.3807 - val_loss: 0.3879\n",
      "Epoch 12/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3806 - val_loss: 0.3913\n",
      "Epoch 13/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3804 - val_loss: 0.3869\n",
      "Epoch 14/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3803 - val_loss: 0.3874\n",
      "Epoch 15/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3799 - val_loss: 0.3866\n",
      "Epoch 16/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3798 - val_loss: 0.3869\n",
      "Epoch 17/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3795 - val_loss: 0.3881\n",
      "Epoch 18/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3794 - val_loss: 0.3864\n",
      "Epoch 19/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3792 - val_loss: 0.3881\n",
      "Epoch 20/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3792 - val_loss: 0.3863\n",
      "Epoch 21/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3790 - val_loss: 0.3862\n",
      "Epoch 22/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3789 - val_loss: 0.3876\n",
      "Epoch 23/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3788 - val_loss: 0.3884\n",
      "Epoch 24/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3788 - val_loss: 0.3874\n",
      "Epoch 25/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3788 - val_loss: 0.3874\n",
      "Epoch 26/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3786 - val_loss: 0.3877\n",
      "Epoch 27/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3786 - val_loss: 0.3878\n",
      "Epoch 28/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3786 - val_loss: 0.3867\n",
      "Epoch 29/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3783 - val_loss: 0.3861\n",
      "Epoch 30/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3785 - val_loss: 0.3868\n",
      "Epoch 31/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3784 - val_loss: 0.3863\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 32/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3777 - val_loss: 0.3860\n",
      "Epoch 33/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3772 - val_loss: 0.3860\n",
      "Epoch 34/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3773 - val_loss: 0.3858\n",
      "Epoch 35/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3773 - val_loss: 0.3859\n",
      "Epoch 36/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3772 - val_loss: 0.3859\n",
      "Epoch 37/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3771 - val_loss: 0.3861\n",
      "Epoch 38/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3773 - val_loss: 0.3862\n",
      "Epoch 39/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3772 - val_loss: 0.3859\n",
      "Epoch 40/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3771 - val_loss: 0.3858\n",
      "Epoch 41/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3772 - val_loss: 0.3857\n",
      "Epoch 42/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3770 - val_loss: 0.3858\n",
      "Epoch 43/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3771 - val_loss: 0.3861\n",
      "Epoch 44/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3768 - val_loss: 0.3857\n",
      "Epoch 45/300\n",
      "9422/9422 [==============================] - 25s 3ms/step - loss: 0.3770 - val_loss: 0.3858\n",
      "Epoch 46/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3771 - val_loss: 0.3858\n",
      "Epoch 47/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3771 - val_loss: 0.3856\n",
      "Epoch 48/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3770 - val_loss: 0.3857\n",
      "Epoch 49/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3770 - val_loss: 0.3859\n",
      "Epoch 50/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3771 - val_loss: 0.3859\n",
      "Epoch 51/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3772 - val_loss: 0.3860\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 52/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3768 - val_loss: 0.3857\n",
      "Epoch 53/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3770 - val_loss: 0.3858\n",
      "Epoch 54/300\n",
      "9422/9422 [==============================] - 24s 2ms/step - loss: 0.3769 - val_loss: 0.3858\n",
      "Epoch 55/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3769 - val_loss: 0.3858\n",
      "Epoch 56/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3769 - val_loss: 0.3858\n",
      "Epoch 57/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3770 - val_loss: 0.3859\n",
      "Epoch 58/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3770 - val_loss: 0.3858\n",
      "Epoch 59/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3770 - val_loss: 0.3859\n",
      "Epoch 60/300\n",
      "9422/9422 [==============================] - 23s 2ms/step - loss: 0.3769 - val_loss: 0.3858\n",
      "Epoch 61/300\n",
      "9422/9422 [==============================] - 24s 2ms/step - loss: 0.3769 - val_loss: 0.3858\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 62/300\n",
      "9422/9422 [==============================] - 24s 3ms/step - loss: 0.3769 - val_loss: 0.3857\n",
      "Epoch 00062: early stopping\n",
      "dca: Calculating reconstructions...\n",
      "dca: Successfully preprocessed 21520 genes and 20939 cells.\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 21520)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           1377344     count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 21520)        1398800     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               multiple             0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 21520)        1398800     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 21520)        0           lambda_2[1][0]                   \n",
      "                                                                 dispersion[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,179,616\n",
      "Trainable params: 4,179,296\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 18845 samples, validate on 2094 samples\n",
      "Epoch 1/300\n",
      "18845/18845 [==============================] - 38s 2ms/step - loss: 0.4292 - val_loss: 0.3892\n",
      "Epoch 2/300\n",
      "18845/18845 [==============================] - 39s 2ms/step - loss: 0.3870 - val_loss: 0.3839\n",
      "Epoch 3/300\n",
      "18845/18845 [==============================] - 39s 2ms/step - loss: 0.3844 - val_loss: 0.3827\n",
      "Epoch 4/300\n",
      "18845/18845 [==============================] - 39s 2ms/step - loss: 0.3830 - val_loss: 0.3815\n",
      "Epoch 5/300\n",
      "18845/18845 [==============================] - 39s 2ms/step - loss: 0.3820 - val_loss: 0.3809\n",
      "Epoch 6/300\n",
      "18845/18845 [==============================] - 41s 2ms/step - loss: 0.3815 - val_loss: 0.3796\n",
      "Epoch 7/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3810 - val_loss: 0.3799\n",
      "Epoch 8/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3806 - val_loss: 0.3796\n",
      "Epoch 9/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3803 - val_loss: 0.3802\n",
      "Epoch 10/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3802 - val_loss: 0.3807\n",
      "Epoch 11/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3800 - val_loss: 0.3819\n",
      "Epoch 12/300\n",
      "18845/18845 [==============================] - 41s 2ms/step - loss: 0.3798 - val_loss: 0.3788\n",
      "Epoch 13/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3795 - val_loss: 0.3786\n",
      "Epoch 14/300\n",
      "18845/18845 [==============================] - 41s 2ms/step - loss: 0.3794 - val_loss: 0.3793\n",
      "Epoch 15/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3793 - val_loss: 0.3790\n",
      "Epoch 16/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3792 - val_loss: 0.3781\n",
      "Epoch 17/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3792 - val_loss: 0.3784\n",
      "Epoch 18/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3791 - val_loss: 0.3793\n",
      "Epoch 19/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3789 - val_loss: 0.3798\n",
      "Epoch 20/300\n",
      "18845/18845 [==============================] - 40s 2ms/step - loss: 0.3788 - val_loss: 0.3807\n",
      "Epoch 21/300\n",
      "18845/18845 [==============================] - 41s 2ms/step - loss: 0.3789 - val_loss: 0.3784\n",
      "Epoch 22/300\n",
      "18845/18845 [==============================] - 41s 2ms/step - loss: 0.3788 - val_loss: 0.3793\n",
      "Epoch 23/300\n",
      "18845/18845 [==============================] - 41s 2ms/step - loss: 0.3786 - val_loss: 0.3789\n",
      "Epoch 24/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3786 - val_loss: 0.3783\n",
      "Epoch 25/300\n",
      "18845/18845 [==============================] - 43s 2ms/step - loss: 0.3786 - val_loss: 0.3786\n",
      "Epoch 26/300\n",
      "18845/18845 [==============================] - 43s 2ms/step - loss: 0.3787 - val_loss: 0.3785\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 27/300\n",
      "18845/18845 [==============================] - 43s 2ms/step - loss: 0.3779 - val_loss: 0.3780\n",
      "Epoch 28/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3778 - val_loss: 0.3781\n",
      "Epoch 29/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3778 - val_loss: 0.3779\n",
      "Epoch 30/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3778 - val_loss: 0.3778\n",
      "Epoch 31/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3777 - val_loss: 0.3780\n",
      "Epoch 32/300\n",
      "18845/18845 [==============================] - 42s 2ms/step - loss: 0.3777 - val_loss: 0.3780\n",
      "Epoch 33/300\n",
      "18845/18845 [==============================] - 43s 2ms/step - loss: 0.3777 - val_loss: 0.3779\n",
      "Epoch 34/300\n",
      "18845/18845 [==============================] - 44s 2ms/step - loss: 0.3776 - val_loss: 0.3779\n",
      "Epoch 35/300\n",
      "18845/18845 [==============================] - 45s 2ms/step - loss: 0.3777 - val_loss: 0.3780\n",
      "Epoch 36/300\n",
      "18845/18845 [==============================] - 45s 2ms/step - loss: 0.3776 - val_loss: 0.3779\n",
      "Epoch 37/300\n",
      "18845/18845 [==============================] - 44s 2ms/step - loss: 0.3777 - val_loss: 0.3781\n",
      "Epoch 38/300\n",
      "18845/18845 [==============================] - 44s 2ms/step - loss: 0.3777 - val_loss: 0.3782\n",
      "Epoch 39/300\n",
      "18845/18845 [==============================] - 44s 2ms/step - loss: 0.3776 - val_loss: 0.3780\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 40/300\n",
      "18845/18845 [==============================] - 43s 2ms/step - loss: 0.3775 - val_loss: 0.3780\n",
      "Epoch 41/300\n",
      "18845/18845 [==============================] - 43s 2ms/step - loss: 0.3776 - val_loss: 0.3780\n",
      "Epoch 42/300\n",
      "18845/18845 [==============================] - 45s 2ms/step - loss: 0.3775 - val_loss: 0.3781\n",
      "Epoch 43/300\n",
      "18845/18845 [==============================] - 45s 2ms/step - loss: 0.3775 - val_loss: 0.3780\n",
      "Epoch 44/300\n",
      "18845/18845 [==============================] - 43s 2ms/step - loss: 0.3776 - val_loss: 0.3781\n",
      "Epoch 45/300\n",
      "18845/18845 [==============================] - 46s 2ms/step - loss: 0.3776 - val_loss: 0.3780\n",
      "Epoch 00045: early stopping\n",
      "dca: Calculating reconstructions...\n",
      "dca: Successfully preprocessed 21521 genes and 41878 cells.\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 21521)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           1377408     count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               multiple             0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 21521)        0           lambda_2[2][0]                   \n",
      "                                                                 dispersion[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,179,810\n",
      "Trainable params: 4,179,490\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 37690 samples, validate on 4188 samples\n",
      "Epoch 1/300\n",
      "37690/37690 [==============================] - 71s 2ms/step - loss: 0.4076 - val_loss: 0.3846\n",
      "Epoch 2/300\n",
      "37690/37690 [==============================] - 73s 2ms/step - loss: 0.3831 - val_loss: 0.3815\n",
      "Epoch 3/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3813 - val_loss: 0.3808\n",
      "Epoch 4/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3805 - val_loss: 0.3801\n",
      "Epoch 5/300\n",
      "37690/37690 [==============================] - 69s 2ms/step - loss: 0.3799 - val_loss: 0.3804\n",
      "Epoch 6/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3796 - val_loss: 0.3795\n",
      "Epoch 7/300\n",
      "37690/37690 [==============================] - 69s 2ms/step - loss: 0.3794 - val_loss: 0.3795\n",
      "Epoch 8/300\n",
      "37690/37690 [==============================] - 72s 2ms/step - loss: 0.3791 - val_loss: 0.3794\n",
      "Epoch 9/300\n",
      "37690/37690 [==============================] - 73s 2ms/step - loss: 0.3790 - val_loss: 0.3793\n",
      "Epoch 10/300\n",
      "37690/37690 [==============================] - 70s 2ms/step - loss: 0.3789 - val_loss: 0.3791\n",
      "Epoch 11/300\n",
      "37690/37690 [==============================] - 69s 2ms/step - loss: 0.3787 - val_loss: 0.3803\n",
      "Epoch 12/300\n",
      "37690/37690 [==============================] - 70s 2ms/step - loss: 0.3788 - val_loss: 0.3794\n",
      "Epoch 13/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3786 - val_loss: 0.3793\n",
      "Epoch 14/300\n",
      "37690/37690 [==============================] - 70s 2ms/step - loss: 0.3787 - val_loss: 0.3795\n",
      "Epoch 15/300\n",
      "37690/37690 [==============================] - 69s 2ms/step - loss: 0.3786 - val_loss: 0.3810\n",
      "Epoch 16/300\n",
      "37690/37690 [==============================] - 72s 2ms/step - loss: 0.3785 - val_loss: 0.3803\n",
      "Epoch 17/300\n",
      "37690/37690 [==============================] - 72s 2ms/step - loss: 0.3786 - val_loss: 0.3798\n",
      "Epoch 18/300\n",
      "37690/37690 [==============================] - 71s 2ms/step - loss: 0.3785 - val_loss: 0.3796\n",
      "Epoch 19/300\n",
      "37690/37690 [==============================] - 73s 2ms/step - loss: 0.3784 - val_loss: 0.3798\n",
      "Epoch 20/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3784 - val_loss: 0.3796\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 21/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3778 - val_loss: 0.3791\n",
      "Epoch 22/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3777 - val_loss: 0.3790\n",
      "Epoch 23/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3777 - val_loss: 0.3790\n",
      "Epoch 24/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3777 - val_loss: 0.3792\n",
      "Epoch 25/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3776 - val_loss: 0.3791\n",
      "Epoch 26/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3776 - val_loss: 0.3789\n",
      "Epoch 27/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3776 - val_loss: 0.3790\n",
      "Epoch 28/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3776 - val_loss: 0.3790\n",
      "Epoch 29/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3776 - val_loss: 0.3789\n",
      "Epoch 30/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3776 - val_loss: 0.3790\n",
      "Epoch 31/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3777 - val_loss: 0.3790\n",
      "Epoch 32/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3775 - val_loss: 0.3789\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 33/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3774 - val_loss: 0.3788\n",
      "Epoch 34/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3775 - val_loss: 0.3789\n",
      "Epoch 35/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3775 - val_loss: 0.3788\n",
      "Epoch 36/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3774 - val_loss: 0.3788\n",
      "Epoch 37/300\n",
      "37690/37690 [==============================] - 68s 2ms/step - loss: 0.3775 - val_loss: 0.3789\n",
      "Epoch 38/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3775 - val_loss: 0.3791\n",
      "Epoch 39/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3774 - val_loss: 0.3788\n",
      "Epoch 40/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3775 - val_loss: 0.3788\n",
      "Epoch 41/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3775 - val_loss: 0.3791\n",
      "Epoch 42/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3774 - val_loss: 0.3789\n",
      "Epoch 43/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3775 - val_loss: 0.3789\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 44/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3775 - val_loss: 0.3789\n",
      "Epoch 45/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3774 - val_loss: 0.3789\n",
      "Epoch 46/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3775 - val_loss: 0.3789\n",
      "Epoch 47/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3774 - val_loss: 0.3789\n",
      "Epoch 48/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3775 - val_loss: 0.3790\n",
      "Epoch 49/300\n",
      "37690/37690 [==============================] - 67s 2ms/step - loss: 0.3774 - val_loss: 0.3789\n",
      "Epoch 50/300\n",
      "37690/37690 [==============================] - 66s 2ms/step - loss: 0.3775 - val_loss: 0.3788\n",
      "Epoch 00050: early stopping\n",
      "dca: Calculating reconstructions...\n",
      "dca: Successfully preprocessed 21521 genes and 62816 cells.\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 21521)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           1377408     count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               multiple             0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 21521)        0           lambda_2[3][0]                   \n",
      "                                                                 dispersion[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,179,810\n",
      "Trainable params: 4,179,490\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 56534 samples, validate on 6282 samples\n",
      "Epoch 1/300\n",
      "56534/56534 [==============================] - 104s 2ms/step - loss: 0.4005 - val_loss: 0.3811\n",
      "Epoch 2/300\n",
      "56534/56534 [==============================] - 100s 2ms/step - loss: 0.3826 - val_loss: 0.3812\n",
      "Epoch 3/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3813 - val_loss: 0.3783\n",
      "Epoch 4/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3807 - val_loss: 0.3782\n",
      "Epoch 5/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3803 - val_loss: 0.3775\n",
      "Epoch 6/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3800 - val_loss: 0.3781\n",
      "Epoch 7/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3798 - val_loss: 0.3781\n",
      "Epoch 8/300\n",
      "56534/56534 [==============================] - 100s 2ms/step - loss: 0.3796 - val_loss: 0.3776\n",
      "Epoch 9/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3796 - val_loss: 0.3777\n",
      "Epoch 10/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3795 - val_loss: 0.3776\n",
      "Epoch 11/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3794 - val_loss: 0.3776\n",
      "Epoch 12/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3795 - val_loss: 0.3779\n",
      "Epoch 13/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3795 - val_loss: 0.3782\n",
      "Epoch 14/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3795 - val_loss: 0.3785\n",
      "Epoch 15/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3794 - val_loss: 0.3775\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3788 - val_loss: 0.3776\n",
      "Epoch 17/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3788 - val_loss: 0.3774\n",
      "Epoch 18/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3786 - val_loss: 0.3773\n",
      "Epoch 19/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3787 - val_loss: 0.3774\n",
      "Epoch 20/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3787 - val_loss: 0.3774\n",
      "Epoch 21/300\n",
      "56534/56534 [==============================] - 100s 2ms/step - loss: 0.3787 - val_loss: 0.3773\n",
      "Epoch 22/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3786 - val_loss: 0.3776\n",
      "Epoch 23/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3786 - val_loss: 0.3774\n",
      "Epoch 24/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3785 - val_loss: 0.3773\n",
      "Epoch 25/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3786 - val_loss: 0.3774\n",
      "Epoch 26/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3786 - val_loss: 0.3776\n",
      "Epoch 27/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3786 - val_loss: 0.3773\n",
      "Epoch 28/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3785 - val_loss: 0.3774\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 29/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3785 - val_loss: 0.3774\n",
      "Epoch 30/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3785 - val_loss: 0.3774\n",
      "Epoch 31/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3785 - val_loss: 0.3773\n",
      "Epoch 32/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3784 - val_loss: 0.3774\n",
      "Epoch 33/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3784 - val_loss: 0.3774\n",
      "Epoch 34/300\n",
      "56534/56534 [==============================] - 100s 2ms/step - loss: 0.3785 - val_loss: 0.3775\n",
      "Epoch 35/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3786 - val_loss: 0.3775\n",
      "Epoch 36/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3785 - val_loss: 0.3775\n",
      "Epoch 37/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3784 - val_loss: 0.3775\n",
      "Epoch 38/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3785 - val_loss: 0.3775\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 39/300\n",
      "56534/56534 [==============================] - 101s 2ms/step - loss: 0.3784 - val_loss: 0.3774\n",
      "Epoch 00039: early stopping\n",
      "dca: Calculating reconstructions...\n",
      "dca: Successfully preprocessed 21521 genes and 83755 cells.\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 21521)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           1377408     count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               multiple             0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 21521)        0           lambda_2[4][0]                   \n",
      "                                                                 dispersion[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,179,810\n",
      "Trainable params: 4,179,490\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 75379 samples, validate on 8376 samples\n",
      "Epoch 1/300\n",
      "75379/75379 [==============================] - 198s 3ms/step - loss: 0.3961 - val_loss: 0.3801\n",
      "Epoch 2/300\n",
      "75379/75379 [==============================] - 144s 2ms/step - loss: 0.3819 - val_loss: 0.3780\n",
      "Epoch 3/300\n",
      "75379/75379 [==============================] - 146s 2ms/step - loss: 0.3807 - val_loss: 0.3771\n",
      "Epoch 4/300\n",
      "75379/75379 [==============================] - 146s 2ms/step - loss: 0.3801 - val_loss: 0.3775\n",
      "Epoch 5/300\n",
      "75379/75379 [==============================] - 150s 2ms/step - loss: 0.3799 - val_loss: 0.3765\n",
      "Epoch 6/300\n",
      "75379/75379 [==============================] - 150s 2ms/step - loss: 0.3797 - val_loss: 0.3768\n",
      "Epoch 7/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3795 - val_loss: 0.3781\n",
      "Epoch 8/300\n",
      "75379/75379 [==============================] - 153s 2ms/step - loss: 0.3795 - val_loss: 0.3768\n",
      "Epoch 9/300\n",
      "75379/75379 [==============================] - 150s 2ms/step - loss: 0.3794 - val_loss: 0.3773\n",
      "Epoch 10/300\n",
      "75379/75379 [==============================] - 151s 2ms/step - loss: 0.3793 - val_loss: 0.3781\n",
      "Epoch 11/300\n",
      "75379/75379 [==============================] - 151s 2ms/step - loss: 0.3793 - val_loss: 0.3770\n",
      "Epoch 12/300\n",
      "75379/75379 [==============================] - 151s 2ms/step - loss: 0.3792 - val_loss: 0.3774\n",
      "Epoch 13/300\n",
      "75379/75379 [==============================] - 152s 2ms/step - loss: 0.3792 - val_loss: 0.3773\n",
      "Epoch 14/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3792 - val_loss: 0.3781\n",
      "Epoch 15/300\n",
      "75379/75379 [==============================] - 153s 2ms/step - loss: 0.3791 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/300\n",
      "75379/75379 [==============================] - 152s 2ms/step - loss: 0.3787 - val_loss: 0.3764\n",
      "Epoch 17/300\n",
      "75379/75379 [==============================] - 155s 2ms/step - loss: 0.3786 - val_loss: 0.3763\n",
      "Epoch 18/300\n",
      "75379/75379 [==============================] - 155s 2ms/step - loss: 0.3785 - val_loss: 0.3763\n",
      "Epoch 19/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3784 - val_loss: 0.3768\n",
      "Epoch 20/300\n",
      "75379/75379 [==============================] - 153s 2ms/step - loss: 0.3784 - val_loss: 0.3764\n",
      "Epoch 21/300\n",
      "75379/75379 [==============================] - 152s 2ms/step - loss: 0.3784 - val_loss: 0.3762\n",
      "Epoch 22/300\n",
      "75379/75379 [==============================] - 152s 2ms/step - loss: 0.3783 - val_loss: 0.3763\n",
      "Epoch 23/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3783 - val_loss: 0.3763\n",
      "Epoch 24/300\n",
      "75379/75379 [==============================] - 153s 2ms/step - loss: 0.3784 - val_loss: 0.3763\n",
      "Epoch 25/300\n",
      "75379/75379 [==============================] - 155s 2ms/step - loss: 0.3783 - val_loss: 0.3764\n",
      "Epoch 26/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3784 - val_loss: 0.3768\n",
      "Epoch 27/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3783 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 28/300\n",
      "75379/75379 [==============================] - 152s 2ms/step - loss: 0.3783 - val_loss: 0.3765\n",
      "Epoch 29/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3783 - val_loss: 0.3764\n",
      "Epoch 30/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3783 - val_loss: 0.3764\n",
      "Epoch 31/300\n",
      "75379/75379 [==============================] - 155s 2ms/step - loss: 0.3783 - val_loss: 0.3764\n",
      "Epoch 32/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3782 - val_loss: 0.3764\n",
      "Epoch 33/300\n",
      "75379/75379 [==============================] - 154s 2ms/step - loss: 0.3783 - val_loss: 0.3765\n",
      "Epoch 34/300\n",
      "75379/75379 [==============================] - 155s 2ms/step - loss: 0.3782 - val_loss: 0.3764\n",
      "Epoch 35/300\n",
      "75379/75379 [==============================] - 164s 2ms/step - loss: 0.3783 - val_loss: 0.3764\n",
      "Epoch 36/300\n",
      "75379/75379 [==============================] - 159s 2ms/step - loss: 0.3783 - val_loss: 0.3765\n",
      "Epoch 00036: early stopping\n",
      "dca: Calculating reconstructions...\n",
      "dca: Successfully preprocessed 21521 genes and 104694 cells.\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 21521)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           1377408     count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               multiple             0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 21521)        1398865     dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 21521)        0           lambda_2[5][0]                   \n",
      "                                                                 dispersion[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,179,810\n",
      "Trainable params: 4,179,490\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 94224 samples, validate on 10470 samples\n",
      "Epoch 1/300\n",
      "94224/94224 [==============================] - 312s 3ms/step - loss: 0.3930 - val_loss: 0.3824\n",
      "Epoch 2/300\n",
      "94224/94224 [==============================] - 168s 2ms/step - loss: 0.3810 - val_loss: 0.3818\n",
      "Epoch 3/300\n",
      "94224/94224 [==============================] - 168s 2ms/step - loss: 0.3800 - val_loss: 0.3804\n",
      "Epoch 4/300\n",
      "94224/94224 [==============================] - 167s 2ms/step - loss: 0.3797 - val_loss: 0.3791\n",
      "Epoch 5/300\n",
      "94224/94224 [==============================] - 176s 2ms/step - loss: 0.3794 - val_loss: 0.3798\n",
      "Epoch 6/300\n",
      "94224/94224 [==============================] - 182s 2ms/step - loss: 0.3792 - val_loss: 0.3794\n",
      "Epoch 7/300\n",
      "94224/94224 [==============================] - 181s 2ms/step - loss: 0.3791 - val_loss: 0.3799\n",
      "Epoch 8/300\n",
      "94224/94224 [==============================] - 181s 2ms/step - loss: 0.3791 - val_loss: 0.3790\n",
      "Epoch 9/300\n",
      "94224/94224 [==============================] - 182s 2ms/step - loss: 0.3789 - val_loss: 0.3812\n",
      "Epoch 10/300\n",
      "94224/94224 [==============================] - 182s 2ms/step - loss: 0.3790 - val_loss: 0.3794\n",
      "Epoch 11/300\n",
      "94224/94224 [==============================] - 181s 2ms/step - loss: 0.3789 - val_loss: 0.3794\n",
      "Epoch 12/300\n",
      "94224/94224 [==============================] - 176s 2ms/step - loss: 0.3789 - val_loss: 0.3800\n",
      "Epoch 13/300\n",
      "94224/94224 [==============================] - 168s 2ms/step - loss: 0.3790 - val_loss: 0.3794\n",
      "Epoch 14/300\n",
      "94224/94224 [==============================] - 167s 2ms/step - loss: 0.3790 - val_loss: 0.3809\n",
      "Epoch 15/300\n",
      "94224/94224 [==============================] - 170s 2ms/step - loss: 0.3791 - val_loss: 0.3808\n",
      "Epoch 16/300\n",
      "94224/94224 [==============================] - 168s 2ms/step - loss: 0.3790 - val_loss: 0.3807\n",
      "Epoch 17/300\n",
      "94224/94224 [==============================] - 165s 2ms/step - loss: 0.3791 - val_loss: 0.3804\n",
      "Epoch 18/300\n",
      "94224/94224 [==============================] - 168s 2ms/step - loss: 0.3790 - val_loss: 0.3812\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/300\n",
      "94224/94224 [==============================] - 169s 2ms/step - loss: 0.3785 - val_loss: 0.3798\n",
      "Epoch 20/300\n",
      "94224/94224 [==============================] - 169s 2ms/step - loss: 0.3784 - val_loss: 0.3798\n",
      "Epoch 21/300\n",
      "94224/94224 [==============================] - 172s 2ms/step - loss: 0.3784 - val_loss: 0.3799\n",
      "Epoch 22/300\n",
      "94224/94224 [==============================] - 169s 2ms/step - loss: 0.3784 - val_loss: 0.3797\n",
      "Epoch 23/300\n",
      "94224/94224 [==============================] - 170s 2ms/step - loss: 0.3784 - val_loss: 0.3796\n",
      "Epoch 00023: early stopping\n",
      "dca: Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "fracs = [0.1, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "for frac in fracs:\n",
    "    np.random.seed(11111)\n",
    "    indices = np.random.choice(range(adata.shape[0]), size = round(frac * adata.shape[0]), replace = False)\n",
    "    pd.DataFrame(indices).to_csv(\"indices\" + str(frac) + \".csv\")\n",
    "\n",
    "n = 0\n",
    "for frac in fracs:\n",
    "    profile_stats.loc[n] = profile(frac)\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (Seconds)</th>\n",
       "      <th>Memory (MiB)</th>\n",
       "      <th>Method</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1506.398011</td>\n",
       "      <td>10378.695312</td>\n",
       "      <td>DCA</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1926.557420</td>\n",
       "      <td>13263.257812</td>\n",
       "      <td>DCA</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3517.835267</td>\n",
       "      <td>20978.339844</td>\n",
       "      <td>DCA</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4168.908276</td>\n",
       "      <td>23393.437500</td>\n",
       "      <td>DCA</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6336.861966</td>\n",
       "      <td>25850.035156</td>\n",
       "      <td>DCA</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5383.067867</td>\n",
       "      <td>24373.273438</td>\n",
       "      <td>DCA</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time (Seconds)  Memory (MiB) Method  Percent\n",
       "0     1506.398011  10378.695312    DCA     10.0\n",
       "1     1926.557420  13263.257812    DCA     20.0\n",
       "2     3517.835267  20978.339844    DCA     40.0\n",
       "3     4168.908276  23393.437500    DCA     60.0\n",
       "4     6336.861966  25850.035156    DCA     80.0\n",
       "5     5383.067867  24373.273438    DCA    100.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_stats.to_csv(\"../Figures/liver/DCA_profile.csv\")\n",
    "profile_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cardec_alternatives]",
   "language": "python",
   "name": "conda-env-cardec_alternatives-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
